{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Build_Week_2_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO0OK2pLcy56sx1EkR2yUmP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KyleTy1er/Build-Week-Unit-2/blob/master/Build_Week_2_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdjZV7vrFrTM",
        "colab_type": "code",
        "outputId": "d331d205-7f92-484f-a043-85837e20a160",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pip install category_encoders\n",
        "print ('clear')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-9-a43af85b40a9>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-a43af85b40a9>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install category_encoders\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p75mOV56DE2W",
        "colab_type": "code",
        "outputId": "3e1dff1c-f3c8-4a55-e423-b24c4a436704",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "# pip install PDPbox\n",
        "print ('clear')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "clear\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_RY_m0a_VWy",
        "colab_type": "code",
        "outputId": "eb672e14-8401-4dee-e99e-25bfbb75e86f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# pip install eli5"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkRpO3b6Vnui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import category_encoders as ce\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'category_encoders'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2099c9c9074d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcategory_encoders\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE9UX-58WW50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/KyleTy1er/Build-Week-Unit-2/master/fighters_df.csv', index_col=0)\n",
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6Q3BsZDUmEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.options.display.max_seq_items = None\n",
        "pd.options.display.max_columns = 999"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcVbeJoU9U_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = train_test_split(df)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44nptSaAE9GF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drop_cols = [\"fighter\", \"fighter_opponent\", \"age\", \"age_opponent\", \"age_ratio\", \"Referee\", \"date\", \"Reach_cms\", \"Reach_cms_opponent\"]\n",
        "\n",
        "target = \"Stance\"\n",
        "features = df.columns.drop([target] + drop_cols)\n",
        "\n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "\n",
        "X_test = test[features]\n",
        "y_test = test[target]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf7U5LL0GwCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.fillna(X_train.mean())\n",
        "y_train = y_train.fillna(method='ffill')\n",
        "X_test = X_test.fillna(X_train.mean())\n",
        "y_test = y_test.fillna(method='ffill')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCrQSiTX98rx",
        "colab_type": "code",
        "outputId": "5bf78f4c-7342-46a2-d729-cb5c3d648507",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "print (y_train.shape)\n",
        "print (X_test.shape)\n",
        "print (y_test.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(7716, 214)\n(7716,)\n(2572, 214)\n(2572,)\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi_cS4NpCuxB",
        "colab_type": "code",
        "outputId": "7e46a15d-04e4-4ec8-bafd-12686588a79c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "baseline_acc = train['Stance'].value_counts(normalize=True)[0].round(2)\n",
        "print (\"Majority Class - Orthodox:\",baseline_acc)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Majority Class - Orthodox: 0.77\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72ybrVy3AQJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = lr_pipeline.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSGpAQRmAUX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_score = accuracy_score(y_pred, y_test)*100"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_pred' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-29-a0ddc6351ea9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlr_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTfT-IAkAXCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "improvement = lr_score - baseline_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoHg9VFbw610",
        "colab_type": "code",
        "outputId": "3e9e0ea4-ed18-4be2-95ee-5675266efd7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "lr_pipeline = make_pipeline(\n",
        "    ce.OneHotEncoder(use_cat_names=True),\n",
        "    SimpleImputer(strategy=\"constant\", fill_value=\"0\"),\n",
        "    LogisticRegression(max_iter=200),\n",
        ")\n",
        "\n",
        "lr_pipeline.fit(X_train, y_train)\n",
        "# y_pred = lr_pipeline.predict(X_test)\n",
        "\n",
        "# lr_score = accuracy_score(y_pred, y_test)*100\n",
        "# improvement = lr_score - baseline_acc"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input contains NaN",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-25-1d8bfbc9de07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mlr_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m# y_pred = lr_pipeline.predict(X_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    354\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[0;32m    355\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[1;32m-> 1532\u001b[1;33m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[0;32m   1533\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1534\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 725\u001b[1;33m         \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    726\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'O'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input contains NaN\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: Input contains NaN"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK3GnqJmAZj8",
        "colab_type": "code",
        "outputId": "1b408934-3d57-494b-d603-afbdd94d00ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print (lr_score)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'lr_score' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-33-383dbdc2fc96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlr_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'lr_score' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTIhulSzwrK9",
        "colab_type": "code",
        "outputId": "16a855a6-aef5-40b2-89f3-efda3efccb50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import category_encoders as ce\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    ce.OneHotEncoder(use_cat_names=True),\n",
        "    SimpleImputer(strategy=\"constant\", fill_value=0),\n",
        "    RandomForestClassifier(n_jobs=-1, n_estimators=220),\n",
        ")\n",
        "\n",
        "# Fit on train, score on val\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "rfc_score = accuracy_score(y_pred, y_test)*100\n",
        "# print('Test Accuracy', pipeline.score(X_train, y_test))\n",
        "print (rfc_score)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input contains NaN",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-36-3494638d4c08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Fit on train, score on val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    354\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[0;32m    355\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 542\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input contains NaN\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: Input contains NaN"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqGGJ_7I_MuT",
        "colab_type": "code",
        "outputId": "8afa7c15-e0e4-4f88-dcea-076c2fee6648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "transformer = make_pipeline(\n",
        "    ce.OrdinalEncoder(),\n",
        "    SimpleImputer(strategy=\"constant\", fill_value=0),\n",
        ")\n",
        "\n",
        "X_train_transformed = transformer.fit_transform(X_train)\n",
        "X_test_transformed = transformer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3l5hA29AESE",
        "colab_type": "code",
        "outputId": "6e14764f-f110-4b0e-b2a1-ca3bcf88bb2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "rfc_model = RandomForestClassifier(\n",
        "    n_jobs=-1,\n",
        "    n_estimators=220,\n",
        ")\n",
        "rfc_model.fit(X_train_transformed, y_train)\n",
        "\n",
        "permuter = PermutationImportance(\n",
        "    rfc_model,\n",
        "    scoring=\"accuracy\",\n",
        "    n_iter=5,\n",
        ")\n",
        "\n",
        "permuter.fit(X_test_transformed, y_test)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input contains NaN",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-40-b27167f04ed4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m220\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m )\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mrfc_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_transformed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m permuter = PermutationImportance(\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 542\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input contains NaN\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: Input contains NaN"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo2NTldzARAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "feature_importances = pd.Series(permuter.feature_importances_, features).sort_values()\n",
        "feature_importances_std = pd.Series(permuter.feature_importances_std_, features).sort_values()\n",
        "\n",
        "good_mask = feature_importances > 0\n",
        "bad_mask = feature_importances < 0\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 30))\n",
        "\n",
        "feature_importances[good_mask].plot.barh(color=\"green\", ecolor=\"#105010\", xerr=feature_importances_std[good_mask], ax=ax)\n",
        "ax.set_title(\"Positive Features\")\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'permuter' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-43-49679497d207>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfeature_importances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpermuter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mfeature_importances_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpermuter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_std_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'permuter' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p5THFOYAZES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 30))\n",
        "\n",
        "feature_importances[bad_mask].plot.barh(color=\"red\", ecolor=\"#501010\", xerr=feature_importances_std[bad_mask], ax=ax)\n",
        "ax.set_title(\"Negative Features\")\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'feature_importances' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-46-68e42b3bb440>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfeature_importances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbad_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"red\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mecolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"#501010\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_importances_std\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbad_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Negative Features\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'feature_importances' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LHu05pVDW55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_transformed = pd.DataFrame(X_test_transformed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTFneMViDdtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_transformed"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   1.5       24        6   \n2570  148.455  71.6364  14.7273  11.9091  12.2727       10  81.4545  28.2727   \n2571  93.7534  51.4419       18  13.6667        9  7.66667       38  16.3333   \n\n            38        39       40       41        42       43       44  \\\n0      6.80935   4.39742   52.023  17.1109  0.155104  5.97988   4.6696   \n1      6.80935   4.39742   52.023  17.1109  0.155104  5.97988   4.6696   \n2          5.8       3.8     65.6     17.6         0     10.6      7.2   \n3            8         6  138.333  71.6667         0  12.6667  12.3333   \n4      6.80935   4.39742   52.023  17.1109  0.155104  5.97988   4.6696   \n...        ...       ...      ...      ...       ...      ...      ...   \n2567       2.8       2.6     90.4     17.6         0     10.4      8.6   \n2568      6.25    3.8125  84.9375  25.0625     0.125    7.875   6.0625   \n2569       3.5       1.8     22.5      3.9         0      5.6        4   \n2570         3   2.54545  78.4545  25.7273         0  3.54545  3.18182   \n2571  0.333333  0.333333       21  4.66667         0  8.33333        6   \n\n            45         46       47       48        49       50       51  \\\n0      1.04796   0.157535  66.5621  27.7264  0.426225  65.9586  27.0057   \n1      1.04796   0.157535  66.5621  27.7264  0.426225  65.9586  27.0057   \n2          3.8        0.4       96     40.8     0.422  65.9586  27.0057   \n3     0.666667          0  66.5621  27.7264  0.426225  170.333  99.3333   \n4      1.04796   0.157535  66.5621  27.7264  0.426225  65.9586  27.0057   \n...        ...        ...      ...      ...       ...      ...      ...   \n2567       0.6          0    116.4     33.2     0.312  65.9586  27.0057   \n2568     0.375          0  66.5621  27.7264  0.426225    102.5    36.25   \n2569       0.6          0  66.5621  27.7264  0.426225     30.2      9.3   \n2570  0.727273  0.0909091  66.5621  27.7264  0.426225  96.7273  40.8182   \n2571   1.33333          0  47.3333  24.3333      0.39  65.9586  27.0057   \n\n            52        53        54        55        56        57       58  \\\n0     0.414369  0.470368  0.453785   2.86408    1.0574  0.266356  86.8973   \n1     0.414369  0.470368  0.453785   2.86408    1.0574  0.266356  86.8973   \n2     0.414369       0.8  0.453785       8.4         3      0.21    161.6   \n3         0.57  0.470368         0  0.666667  0.333333  0.166667  86.8973   \n4     0.414369  0.470368  0.453785   2.86408    1.0574  0.266356  86.8973   \n...        ...       ...       ...       ...       ...       ...      ...   \n2567  0.414369       0.6  0.453785       4.6       1.2     0.128    124.2   \n2568  0.378125  0.470368    0.3125    3.1875    0.4375  0.214375  86.8973   \n2569     0.278  0.470368         0       2.9       0.7       0.2  86.8973   \n2570  0.470909  0.470368  0.272727         5   1.54545  0.336364  86.8973   \n2571  0.414369  0.666667  0.453785         1  0.666667      0.22  57.6667   \n\n           59       60       61 62  63 64     65   66  67 68 69      70 71  \\\n0     45.9246  86.3832  45.2787  0   0  0   True    1   0  0  3  193.04  1   \n1     45.9246  86.3832  45.2787  0   0  0   True   43   0  0  3   177.8  1   \n2        98.2  86.3832  45.2787  0   1  0   True    5   2  2  5  162.56  4   \n3     45.9246  192.333  118.667  2   0  0  False   23   1  2  3   152.4  1   \n4     45.9246  86.3832  45.2787  0   0  0  False   94   0  0  3  185.42  1   \n...       ...      ...      ... ..  .. ..    ...  ...  .. .. ..     ... ..   \n2567       41  86.3832  45.2787  1   0  0   True   27   1  3  3  172.72  1   \n2568  45.9246  110.562       43  0   2  0   True   81   4  4  3   177.8  4   \n2569  45.9246     37.3     15.5  0  10  0   True  103  10  0  5  185.42  4   \n2570  45.9246  122.091  63.7273  2   0  0   True   48   5  4  3  185.42  1   \n2571  34.3333  86.3832  45.2787  0   1  0  False   73   1  2  3  182.88  1   \n\n       72       73       74       75       76       77       78       79  \\\n0     242  8.72425  6.08878  8.18359   5.5174  53.6427  19.5368  9.16017   \n1     170  8.72425  6.08878  8.18359   5.5174  53.6427  19.5368  9.16017   \n2     125       25  15.6667  5.66667  5.66667  94.6667       36  3.66667   \n3     115      5.5      2.5        0        0      132       31        0   \n4     185  3.33333  2.66667        3        2  6.66667        3        4   \n...   ...      ...      ...      ...      ...      ...      ...      ...   \n2567  155    18.96     12.4      9.8     6.88    97.84    36.12     2.04   \n2568  155     15.1      9.8     13.2      8.6     51.5     20.3      7.2   \n2569  170  2.85714  1.85714  2.71429  1.57143  13.2857  4.42857  10.1429   \n2570  135  13.5833      8.5    10.25  5.91667    68.25  19.9167  13.5833   \n2571  155       11        6  3.66667  1.66667  61.6667  20.3333        7   \n\n           80       81       82        83       84        85       86  \\\n0     6.05938  56.0722  20.1333  0.255308  6.18999   4.89149  1.34705   \n1     6.05938  56.0722  20.1333  0.255308  6.18999   4.89149  1.34705   \n2     2.33333       59       18  0.333333       20   10.3333        3   \n3           0      117       22         0      9.5       6.5        0   \n4     3.33333  7.66667  3.66667         0  2.66667         2  2.66667   \n...       ...      ...      ...       ...      ...       ...      ...   \n2567     1.08    80.76    23.48      0.24     9.96       8.2     0.12   \n2568      4.9     50.2     18.5       0.3      6.6       5.5        2   \n2569  6.28571  22.2857  9.71429         0        1  0.714286  2.57143   \n2570      9.5  72.3333  22.5833  0.166667  6.16667      4.25     0.25   \n2571        6  56.6667  17.6667         0  4.66667   4.33333  2.66667   \n\n             87       88       89        90         91       92        93  \\\n0      0.163136  70.9864  31.1135  0.459137   0.547939  2.93032   1.25313   \n1      0.163136  70.9864  31.1135  0.459137   0.547939  2.93032   1.25313   \n2             0      104       44  0.403333   0.333333  9.33333         2   \n3             0      132       31     0.215          0      1.5       0.5   \n4             0  13.6667  8.33333  0.626667          1  5.66667   2.66667   \n...         ...      ...      ...       ...        ...      ...       ...   \n2567       0.04   109.68    44.08    0.4144          0     1.64       0.6   \n2568          0     71.9     33.8      0.48        0.5      3.3       0.9   \n2569   0.428571  26.1429  12.2857  0.491429    1.28571  3.85714   1.57143   \n2570  0.0833333  92.0833  35.3333  0.361667  0.0833333  1.16667  0.666667   \n2571          0  72.3333       28  0.393333   0.333333  5.33333         1   \n\n            94       95       96       97       98       99      100      101  \\\n0     0.326234  93.5388  51.2208  8.25773  5.58494  7.42479  4.87801   52.518   \n1     0.326234  93.5388  51.2208  8.25773  5.58494  7.42479  4.87801   52.518   \n2     0.163333  119.667  57.3333  8.33333  7.33333        4  2.33333  54.6667   \n3          0.5      132       31        8      3.5        0        0      177   \n4     0.306667       29       23  6.66667        5  6.66667  4.66667  8.33333   \n...        ...      ...      ...      ...      ...      ...      ...      ...   \n2567      0.32   120.68    53.88    14.36     9.56    12.24     8.08   114.72   \n2568     0.291     80.8     41.5     15.2      8.6      7.3      4.8     57.2   \n2569      0.42  53.5714  34.5714  3.42857  1.28571  1.57143  1.14286  16.5714   \n2570  0.325833  129.333    69.75  5.33333        4  5.08333  3.16667  35.8333   \n2571  0.213333  97.6667       50       10        7  14.3333  10.3333  72.3333   \n\n          102      103       104      105      106       107      108  \\\n0     18.2152  6.86866   4.46479  52.5339  17.2657  0.156717  6.01974   \n1     18.2152  6.86866   4.46479  52.5339  17.2657  0.156717  6.01974   \n2     22.6667        1  0.666667  48.3333  16.3333         0        3   \n3          24        0         0    165.5     18.5         0      3.5   \n4           3  7.66667         6  13.3333        7         0  2.66667   \n...       ...      ...       ...      ...      ...       ...      ...   \n2567    38.68     2.76      1.76   104.76    30.88      0.28     10.6   \n2568       20      2.4       1.9     47.5     14.4       0.2      4.2   \n2569  3.57143  6.14286   2.85714  18.1429  4.14286  0.142857  2.71429   \n2570  14.4167     7.75   4.16667  37.0833    12.25       0.5     6.25   \n2571  23.6667  7.33333   4.33333  72.6667  23.3333  0.333333  11.3333   \n\n          109       110        111      112      113       114       115  \\\n0     4.70733    1.0529   0.154548  66.8114   27.558  0.419474  0.452124   \n1     4.70733    1.0529   0.154548  66.8114   27.558  0.419474  0.452124   \n2           2         0          0  59.6667  25.6667  0.423333         0   \n3           2       0.5          0      177       24      0.18         0   \n4     1.66667  0.333333          1  22.6667  13.6667      0.59   1.33333   \n...       ...       ...        ...      ...      ...       ...       ...   \n2567     8.08      0.28       0.04   129.72    48.52     0.372      0.08   \n2568      3.7       0.8        0.1     66.9     26.7     0.411       0.6   \n2569  2.14286  0.285714   0.285714  24.2857  7.57143  0.445714  0.571429   \n2570      5.5         2  0.0833333  48.6667    21.75  0.436667  0.333333   \n2571        8         0          0       94  38.3333      0.44         0   \n\n           116       117       118      119      120 121 122 123 124 125 126  \\\n0      2.84955   1.05416  0.265772  87.2149  45.8101   0   0   0   0   0   0   \n1      2.84955   1.05416  0.265772  87.2149  45.8101   0   0   0   0   0   0   \n2     0.666667         0         0       64       29   0   3   0   3   0   8   \n3          0.5         0         0    177.5     24.5   0   1   0   1   1   6   \n4     0.333333  0.333333  0.333333       75  65.3333   0   1   0   1   2   4   \n...        ...       ...       ...      ...      ...  ..  ..  ..  ..  ..  ..   \n2567      3.32       0.8    0.1924      140     57.6   1   0   0   3  13  65   \n2568       1.6       0.4     0.225     82.1     38.9   0   2   0   5   2  21   \n2569  0.428571  0.142857  0.142857  36.7143  19.7143   0   1   0   5   1  12   \n2570   4.66667   1.91667    0.3325  68.8333  40.4167   1   0   0   2   7  28   \n2571         4  0.666667  0.166667  132.667  71.3333   0   1   0   1   1   8   \n\n          127 128 129 130 131 132 133 134 135    136 137      138 139 140 141  \\\n0     598.392   0   0   0   0   0   0   0   0  False   0   596.98   0   4   0   \n1     598.392   0   0   0   0   0   0   0   0  False   0   596.98   0   3   0   \n2     703.667   0   0   0   2   1   0   0   3   True  17     1020   1  11   0   \n3         900   0   0   1   0   0   0   0   1  False   8      746   1   6   0   \n4     638.333   1   0   0   0   0   1   0   1  False   0   596.98   0   5   0   \n...       ...  ..  ..  ..  ..  ..  ..  ..  ..    ...  ..      ...  ..  ..  ..   \n2567   719.72   1   0   2   5   4   0   1  12  False  15      900   0   8   0   \n2568    557.7   0   0   1   3   1   3   0   8  False  38    635.5   0   8   1   \n2569  368.857   0   0   0   1   0   5   0   6   True  19    431.1   6   5   0   \n2570  602.083   0   0   1   2   2   0   0   5  False  30  778.364   0   9   0   \n2571  794.333   1   0   0   1   0   1   0   2  False   6      558   0   8   0   \n\n     142 143 144 145 146 147       148       149       150       151  \\\n0      0   0   0   0   0   0   0.96073   1.00095  0.765432   1.60701   \n1      0   0   0   0   0   0         1   1.00095         1   1.60701   \n2      0   3   0   0   0   3  0.984471   1.01529         1  0.953846   \n3      1   0   0   0   0   1   1.08279   1.11591         1   2.66667   \n4      0   0   0   0   0   0  0.986375   1.00095  0.919355   1.60701   \n...   ..  ..  ..  ..  ..  ..       ...       ...       ...       ...   \n2567   1   1   0   0   0   2   1.02924   1.05764         1  0.761523   \n2568   1   5   5   0   0  12   1.01421   1.04144         1   1.22671   \n2569   0   1   7   2   0  10   1.01363   1.06907   1.08772   2.22963   \n2570   3   3   1   0   0   7  0.918249  0.944747         1  0.866494   \n2571   0   0   0   1   0   1         1  0.973473   1.09615  0.361111   \n\n           152       153       154       155       156       157       158  \\\n0      1.55544   1.85307   1.72106   1.87933   1.75692   2.11033   1.85015   \n1      1.55544   1.85307   1.72106   1.87933   1.75692   2.11033   1.85015   \n2        1.188      3.54      2.73    1.0223   1.05405  0.342857      0.48   \n3      2.66667         8         6  0.598997  0.677083   7.66667   5.66667   \n4      1.55544   1.85307   1.72106   1.87933   1.75692   2.11033   1.85015   \n...        ...       ...       ...       ...       ...       ...       ...   \n2567  0.686567  0.777778   0.71066   1.08256   1.18534  0.526316  0.769231   \n2568   1.13426  0.171655  0.195312   1.88929   1.92195  0.434451  0.423729   \n2569     2.765   2.42308      3.15     1.645   2.92895  0.987179   1.26275   \n2570   1.04306   1.06667   1.07777   1.38234   1.62115   1.05974  0.900433   \n2571   0.52381   1.64286         2  0.574468  0.671875     0.125  0.142857   \n\n           159       160       161       162       163       164       165  \\\n0      1.67306   1.55684   1.05014    1.7786   1.67947   1.27741   1.03717   \n1      1.67306   1.55684   1.05014    1.7786   1.67947   1.27741   1.03717   \n2      1.31333   1.25263       0.9   0.92381   1.34118       0.6       1.4   \n3     0.607345  0.826087         1  0.603175  0.666667   3.66667   1.33333   \n4      1.67306   1.55684   1.05014    1.7786   1.67947   1.27741   1.03717   \n...        ...       ...       ...       ...       ...       ...       ...   \n2567   1.10812   1.33987  0.806452    1.0219         1   1.07143  0.961538   \n2568   1.29883  0.935897   1.15385   2.49178   2.27885     0.375         1   \n2569   1.11656   1.58667       2.1      4.45   4.84167     0.336      0.77   \n2570   1.41446    1.5265   1.09091  0.951374  0.935065       1.6  0.923077   \n2571  0.572254  0.535714         1   1.29412    1.3125  0.818182         1   \n\n           166       167       168       169       170       171       172  \\\n0      1.43033   1.39269   1.00165   1.12131    1.4646   1.23351   1.02838   \n1      1.43033   1.39269   1.00165   1.12131    1.4646   1.23351   1.02838   \n2      1.15238   1.26222   1.05036   1.12131  0.832258   1.53333   1.21547   \n3      1.43033   1.39269   1.00165         1   3.73333   1.55556  0.777778   \n4      1.43033   1.39269   1.00165   1.12131    1.4646   1.23351   1.02838   \n...        ...       ...       ...       ...       ...       ...       ...   \n2567   1.03903   1.09139    1.0082   1.12131   1.43939      1.25  0.984848   \n2568   1.43033   1.39269   1.00165  0.708333  0.421512  0.756579  0.951782   \n2569   1.43033   1.39269   1.00165   0.56875  0.267647  0.466667   0.84507   \n2570   1.43033   1.39269   1.00165   1.00699   1.76224   1.47273   1.04703   \n2571  0.581818  0.643678  0.952153   1.12131  0.842105         1   1.20055   \n\n           173       174       175       176       177       178       179  \\\n0      1.40671    1.4523    1.5148   1.46602   1.79824   1.67377   1.82982   \n1      1.40671    1.4523    1.5148   1.46602   1.79824   1.67377   1.82982   \n2      1.40552   1.68343   2.22857      2.04      4.24      4.98   1.27545   \n3      1.40671    1.4523   2.25926   3.62963   34.3333   22.3333  0.730337   \n4      1.40671    1.4523    1.5148   1.46602   1.79824   1.67377   1.82982   \n...        ...       ...       ...       ...       ...       ...       ...   \n2567   1.00592   1.01312   1.08073  0.757576  0.377644  0.330396  0.955755   \n2568   1.40671    1.4523  0.659722  0.638021  0.263554  0.280172   1.65056   \n2569   1.40671    1.4523       0.7      1.05   1.43889   1.16667   1.42276   \n2570   1.40671    1.4523   2.48325   2.58182   2.18182      2.64   2.23858   \n2571  0.476351  0.437908   1.72727   1.83333  0.652174  0.764706  0.531818   \n\n           180       181       182       183       184       185       186  \\\n0      1.71755   2.23717   1.89585   1.67112   1.60314   1.02981   1.58847   \n1      1.71755   2.23717   1.89585   1.67112   1.60314   1.02981   1.58847   \n2     0.946479       3.4      2.88      1.35   1.07308         1       2.9   \n3         2.92         9         7  0.836837    3.7265         1   3.03704   \n4      1.71755   2.23717   1.89585   1.67112   1.60314   1.02981   1.58847   \n...        ...       ...       ...       ...       ...       ...       ...   \n2567  0.745968   1.01064   1.30435  0.864221  0.583438   0.78125  0.982759   \n2568    1.5625   2.13235   1.65948   1.77191   1.69237    0.9375   1.70673   \n2569   1.53125      0.63  0.725926   1.22761  0.952778     0.875   1.77692   \n2570   1.89877  0.457143  0.686217   2.08633   2.01715  0.666667  0.626959   \n2571  0.702703      0.16      0.25  0.298643  0.232877      0.75  0.756757   \n\n           187       188       189       190       191       192       193  \\\n0      1.52051    1.2442   1.03812   1.56157   1.51032   1.01581   1.10828   \n1      1.52051    1.2442   1.03812   1.56157   1.51032   1.01581   1.10828   \n2      2.73333       4.8       1.4    1.5989    1.5675  0.999063   1.10828   \n3      4.44444   1.11111         1   1.56157   1.51032   1.01581         1   \n4      1.52051    1.2442   1.03812   1.56157   1.51032   1.01581   1.10828   \n...        ...       ...       ...       ...       ...       ...       ...   \n2567   1.05727      1.25  0.961538  0.898103   0.69063  0.956268   1.10828   \n2568   1.50266  0.763889  0.909091   1.56157   1.51032   1.01581  0.820312   \n2569   1.59091   1.24444  0.777778   1.56157   1.51032   1.01581  0.636364   \n2570  0.643357  0.575758   1.00699   1.56157   1.51032   1.01581  0.954545   \n2571  0.777778   2.33333         1  0.508772  0.644068  0.965278   1.10828   \n\n          194       195       196       197       198  199  200 201       202  \\\n0     1.40841   1.20057   1.02702   1.56205   1.61993    1    1   1         1   \n1     1.40841   1.20057   1.02702   1.56205   1.61993    1    1   1         1   \n2        5.64         4      1.21   2.50154   3.30667    1  0.5   1      0.75   \n3     1.11111   1.33333   1.16667   1.56205   1.61993    3  0.5   1         1   \n4     1.40841   1.20057   1.02702   1.56205   1.61993    1  0.5   1       0.5   \n...       ...       ...       ...       ...       ...  ...  ...  ..       ...   \n2567   1.2963   1.22222  0.945991  0.887943  0.716724    1    1   1       0.5   \n2568  1.61058   1.02679  0.991327   1.56205   1.61993    1    1   1  0.833333   \n2569     2.73    1.4875      1.05   1.56205   1.61993    1  5.5   1   1.83333   \n2570  1.05882  0.872727    1.0029   1.56205   1.61993  1.5    1   1         2   \n2571      0.4         1   1.04571  0.438903  0.488479    1    1   1         1   \n\n           203       204       205  206 207       208       209       210  \\\n0            1         1   1.39209    1   1         1         1         1   \n1            1         1   1.39209    1   1         1         1         1   \n2            3         2   1.44891    2   1         1   1.33333       0.5   \n3          1.5   1.28571  0.829079    2   1         1         1         1   \n4     0.333333       0.2   1.39209  0.5   1         1         1         1   \n...        ...       ...       ...  ...  ..       ...       ...       ...   \n2567  0.285714  0.242424   1.25014  0.5   1  0.666667  0.333333       0.2   \n2568   1.66667   1.77273   1.13925    1   2         1       1.5         3   \n2569       0.5   1.53846   1.16829    7   1         1         1         8   \n2570     0.625   1.06897    1.2923    1   1         2   1.33333  0.666667   \n2571       1.5  0.777778   0.70285  0.5   1         1       0.5         1   \n\n       211  212       213  \n0        1    1         1  \n1        1    1         1  \n2        1    1         1  \n3        1    1         1  \n4      0.5    1       0.5  \n...    ...  ...       ...  \n2567     1  0.5  0.230769  \n2568  0.25    1   1.44444  \n2569   0.5    1   1.57143  \n2570     1    1   1.33333  \n2571     1    1  0.666667  \n\n[2572 rows x 214 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>39</th>\n      <th>40</th>\n      <th>41</th>\n      <th>42</th>\n      <th>43</th>\n      <th>44</th>\n      <th>45</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n      <th>50</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>55</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n      <th>60</th>\n      <th>61</th>\n      <th>62</th>\n      <th>63</th>\n      <th>64</th>\n      <th>65</th>\n      <th>66</th>\n      <th>67</th>\n      <th>68</th>\n      <th>69</th>\n      <th>70</th>\n      <th>71</th>\n      <th>72</th>\n      <th>73</th>\n      <th>74</th>\n      <th>75</th>\n      <th>76</th>\n      <th>77</th>\n      <th>78</th>\n      <th>79</th>\n      <th>80</th>\n      <th>81</th>\n      <th>82</th>\n      <th>83</th>\n      <th>84</th>\n      <th>85</th>\n      <th>86</th>\n      <th>87</th>\n      <th>88</th>\n      <th>89</th>\n      <th>90</th>\n      <th>91</th>\n      <th>92</th>\n      <th>93</th>\n      <th>94</th>\n      <th>95</th>\n      <th>96</th>\n      <th>97</th>\n      <th>98</th>\n      <th>99</th>\n      <th>100</th>\n      <th>101</th>\n      <th>102</th>\n      <th>103</th>\n      <th>104</th>\n      <th>105</th>\n      <th>106</th>\n      <th>107</th>\n      <th>108</th>\n      <th>109</th>\n      <th>110</th>\n      <th>111</th>\n      <th>112</th>\n      <th>113</th>\n      <th>114</th>\n      <th>115</th>\n      <th>116</th>\n      <th>117</th>\n      <th>118</th>\n      <th>119</th>\n      <th>120</th>\n      <th>121</th>\n      <th>122</th>\n      <th>123</th>\n      <th>124</th>\n      <th>125</th>\n      <th>126</th>\n      <th>127</th>\n      <th>128</th>\n      <th>129</th>\n      <th>130</th>\n      <th>131</th>\n      <th>132</th>\n      <th>133</th>\n      <th>134</th>\n      <th>135</th>\n      <th>136</th>\n      <th>137</th>\n      <th>138</th>\n      <th>139</th>\n      <th>140</th>\n      <th>141</th>\n      <th>142</th>\n      <th>143</th>\n      <th>144</th>\n      <th>145</th>\n      <th>146</th>\n      <th>147</th>\n      <th>148</th>\n      <th>149</th>\n      <th>150</th>\n      <th>151</th>\n      <th>152</th>\n      <th>153</th>\n      <th>154</th>\n      <th>155</th>\n      <th>156</th>\n      <th>157</th>\n      <th>158</th>\n      <th>159</th>\n      <th>160</th>\n      <th>161</th>\n      <th>162</th>\n      <th>163</th>\n      <th>164</th>\n      <th>165</th>\n      <th>166</th>\n      <th>167</th>\n      <th>168</th>\n      <th>169</th>\n      <th>170</th>\n      <th>171</th>\n      <th>172</th>\n      <th>173</th>\n      <th>174</th>\n      <th>175</th>\n      <th>176</th>\n      <th>177</th>\n      <th>178</th>\n      <th>179</th>\n      <th>180</th>\n      <th>181</th>\n      <th>182</th>\n      <th>183</th>\n      <th>184</th>\n      <th>185</th>\n      <th>186</th>\n      <th>187</th>\n      <th>188</th>\n      <th>189</th>\n      <th>190</th>\n      <th>191</th>\n      <th>192</th>\n      <th>193</th>\n      <th>194</th>\n      <th>195</th>\n      <th>196</th>\n      <th>197</th>\n      <th>198</th>\n      <th>199</th>\n      <th>200</th>\n      <th>201</th>\n      <th>202</th>\n      <th>203</th>\n      <th>204</th>\n      <th>205</th>\n      <th>206</th>\n      <th>207</th>\n      <th>208</th>\n      <th>209</th>\n      <th>210</th>\n      <th>211</th>\n      <th>212</th>\n      <th>213</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>185.42</td>\n      <td>185</td>\n      <td>8.68893</td>\n      <td>6.05741</td>\n      <td>8.08863</td>\n      <td>5.44475</td>\n      <td>53.3634</td>\n      <td>19.4233</td>\n      <td>9.19593</td>\n      <td>6.08155</td>\n      <td>55.8417</td>\n      <td>20.0612</td>\n      <td>0.255765</td>\n      <td>6.11738</td>\n      <td>4.83103</td>\n      <td>1.34854</td>\n      <td>0.160135</td>\n      <td>69.894</td>\n      <td>30.4553</td>\n      <td>0.455581</td>\n      <td>71.2957</td>\n      <td>31.3742</td>\n      <td>0.464392</td>\n      <td>0.550644</td>\n      <td>0.537707</td>\n      <td>2.89827</td>\n      <td>1.2467</td>\n      <td>0.323823</td>\n      <td>92.116</td>\n      <td>50.2807</td>\n      <td>93.7534</td>\n      <td>51.4419</td>\n      <td>8.23462</td>\n      <td>5.55821</td>\n      <td>7.36715</td>\n      <td>4.84363</td>\n      <td>52.061</td>\n      <td>18.0976</td>\n      <td>6.80935</td>\n      <td>4.39742</td>\n      <td>52.023</td>\n      <td>17.1109</td>\n      <td>0.155104</td>\n      <td>5.97988</td>\n      <td>4.6696</td>\n      <td>1.04796</td>\n      <td>0.157535</td>\n      <td>66.5621</td>\n      <td>27.7264</td>\n      <td>0.426225</td>\n      <td>65.9586</td>\n      <td>27.0057</td>\n      <td>0.414369</td>\n      <td>0.470368</td>\n      <td>0.453785</td>\n      <td>2.86408</td>\n      <td>1.0574</td>\n      <td>0.266356</td>\n      <td>86.8973</td>\n      <td>45.9246</td>\n      <td>86.3832</td>\n      <td>45.2787</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>193.04</td>\n      <td>1</td>\n      <td>242</td>\n      <td>8.72425</td>\n      <td>6.08878</td>\n      <td>8.18359</td>\n      <td>5.5174</td>\n      <td>53.6427</td>\n      <td>19.5368</td>\n      <td>9.16017</td>\n      <td>6.05938</td>\n      <td>56.0722</td>\n      <td>20.1333</td>\n      <td>0.255308</td>\n      <td>6.18999</td>\n      <td>4.89149</td>\n      <td>1.34705</td>\n      <td>0.163136</td>\n      <td>70.9864</td>\n      <td>31.1135</td>\n      <td>0.459137</td>\n      <td>0.547939</td>\n      <td>2.93032</td>\n      <td>1.25313</td>\n      <td>0.326234</td>\n      <td>93.5388</td>\n      <td>51.2208</td>\n      <td>8.25773</td>\n      <td>5.58494</td>\n      <td>7.42479</td>\n      <td>4.87801</td>\n      <td>52.518</td>\n      <td>18.2152</td>\n      <td>6.86866</td>\n      <td>4.46479</td>\n      <td>52.5339</td>\n      <td>17.2657</td>\n      <td>0.156717</td>\n      <td>6.01974</td>\n      <td>4.70733</td>\n      <td>1.0529</td>\n      <td>0.154548</td>\n      <td>66.8114</td>\n      <td>27.558</td>\n      <td>0.419474</td>\n      <td>0.452124</td>\n      <td>2.84955</td>\n      <td>1.05416</td>\n      <td>0.265772</td>\n      <td>87.2149</td>\n      <td>45.8101</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>598.392</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>0</td>\n      <td>596.98</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.96073</td>\n      <td>1.00095</td>\n      <td>0.765432</td>\n      <td>1.60701</td>\n      <td>1.55544</td>\n      <td>1.85307</td>\n      <td>1.72106</td>\n      <td>1.87933</td>\n      <td>1.75692</td>\n      <td>2.11033</td>\n      <td>1.85015</td>\n      <td>1.67306</td>\n      <td>1.55684</td>\n      <td>1.05014</td>\n      <td>1.7786</td>\n      <td>1.67947</td>\n      <td>1.27741</td>\n      <td>1.03717</td>\n      <td>1.43033</td>\n      <td>1.39269</td>\n      <td>1.00165</td>\n      <td>1.12131</td>\n      <td>1.4646</td>\n      <td>1.23351</td>\n      <td>1.02838</td>\n      <td>1.40671</td>\n      <td>1.4523</td>\n      <td>1.5148</td>\n      <td>1.46602</td>\n      <td>1.79824</td>\n      <td>1.67377</td>\n      <td>1.82982</td>\n      <td>1.71755</td>\n      <td>2.23717</td>\n      <td>1.89585</td>\n      <td>1.67112</td>\n      <td>1.60314</td>\n      <td>1.02981</td>\n      <td>1.58847</td>\n      <td>1.52051</td>\n      <td>1.2442</td>\n      <td>1.03812</td>\n      <td>1.56157</td>\n      <td>1.51032</td>\n      <td>1.01581</td>\n      <td>1.10828</td>\n      <td>1.40841</td>\n      <td>1.20057</td>\n      <td>1.02702</td>\n      <td>1.56205</td>\n      <td>1.61993</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1.39209</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>177.8</td>\n      <td>170</td>\n      <td>8.68893</td>\n      <td>6.05741</td>\n      <td>8.08863</td>\n      <td>5.44475</td>\n      <td>53.3634</td>\n      <td>19.4233</td>\n      <td>9.19593</td>\n      <td>6.08155</td>\n      <td>55.8417</td>\n      <td>20.0612</td>\n      <td>0.255765</td>\n      <td>6.11738</td>\n      <td>4.83103</td>\n      <td>1.34854</td>\n      <td>0.160135</td>\n      <td>69.894</td>\n      <td>30.4553</td>\n      <td>0.455581</td>\n      <td>71.2957</td>\n      <td>31.3742</td>\n      <td>0.464392</td>\n      <td>0.550644</td>\n      <td>0.537707</td>\n      <td>2.89827</td>\n      <td>1.2467</td>\n      <td>0.323823</td>\n      <td>92.116</td>\n      <td>50.2807</td>\n      <td>93.7534</td>\n      <td>51.4419</td>\n      <td>8.23462</td>\n      <td>5.55821</td>\n      <td>7.36715</td>\n      <td>4.84363</td>\n      <td>52.061</td>\n      <td>18.0976</td>\n      <td>6.80935</td>\n      <td>4.39742</td>\n      <td>52.023</td>\n      <td>17.1109</td>\n      <td>0.155104</td>\n      <td>5.97988</td>\n      <td>4.6696</td>\n      <td>1.04796</td>\n      <td>0.157535</td>\n      <td>66.5621</td>\n      <td>27.7264</td>\n      <td>0.426225</td>\n      <td>65.9586</td>\n      <td>27.0057</td>\n      <td>0.414369</td>\n      <td>0.470368</td>\n      <td>0.453785</td>\n      <td>2.86408</td>\n      <td>1.0574</td>\n      <td>0.266356</td>\n      <td>86.8973</td>\n      <td>45.9246</td>\n      <td>86.3832</td>\n      <td>45.2787</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n      <td>43</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>177.8</td>\n      <td>1</td>\n      <td>170</td>\n      <td>8.72425</td>\n      <td>6.08878</td>\n      <td>8.18359</td>\n      <td>5.5174</td>\n      <td>53.6427</td>\n      <td>19.5368</td>\n      <td>9.16017</td>\n      <td>6.05938</td>\n      <td>56.0722</td>\n      <td>20.1333</td>\n      <td>0.255308</td>\n      <td>6.18999</td>\n      <td>4.89149</td>\n      <td>1.34705</td>\n      <td>0.163136</td>\n      <td>70.9864</td>\n      <td>31.1135</td>\n      <td>0.459137</td>\n      <td>0.547939</td>\n      <td>2.93032</td>\n      <td>1.25313</td>\n      <td>0.326234</td>\n      <td>93.5388</td>\n      <td>51.2208</td>\n      <td>8.25773</td>\n      <td>5.58494</td>\n      <td>7.42479</td>\n      <td>4.87801</td>\n      <td>52.518</td>\n      <td>18.2152</td>\n      <td>6.86866</td>\n      <td>4.46479</td>\n      <td>52.5339</td>\n      <td>17.2657</td>\n      <td>0.156717</td>\n      <td>6.01974</td>\n      <td>4.70733</td>\n      <td>1.0529</td>\n      <td>0.154548</td>\n      <td>66.8114</td>\n      <td>27.558</td>\n      <td>0.419474</td>\n      <td>0.452124</td>\n      <td>2.84955</td>\n      <td>1.05416</td>\n      <td>0.265772</td>\n      <td>87.2149</td>\n      <td>45.8101</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>598.392</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>0</td>\n      <td>596.98</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.00095</td>\n      <td>1</td>\n      <td>1.60701</td>\n      <td>1.55544</td>\n      <td>1.85307</td>\n      <td>1.72106</td>\n      <td>1.87933</td>\n      <td>1.75692</td>\n      <td>2.11033</td>\n      <td>1.85015</td>\n      <td>1.67306</td>\n      <td>1.55684</td>\n      <td>1.05014</td>\n      <td>1.7786</td>\n      <td>1.67947</td>\n      <td>1.27741</td>\n      <td>1.03717</td>\n      <td>1.43033</td>\n      <td>1.39269</td>\n      <td>1.00165</td>\n      <td>1.12131</td>\n      <td>1.4646</td>\n      <td>1.23351</td>\n      <td>1.02838</td>\n      <td>1.40671</td>\n      <td>1.4523</td>\n      <td>1.5148</td>\n      <td>1.46602</td>\n      <td>1.79824</td>\n      <td>1.67377</td>\n      <td>1.82982</td>\n      <td>1.71755</td>\n      <td>2.23717</td>\n      <td>1.89585</td>\n      <td>1.67112</td>\n      <td>1.60314</td>\n      <td>1.02981</td>\n      <td>1.58847</td>\n      <td>1.52051</td>\n      <td>1.2442</td>\n      <td>1.03812</td>\n      <td>1.56157</td>\n      <td>1.51032</td>\n      <td>1.01581</td>\n      <td>1.10828</td>\n      <td>1.40841</td>\n      <td>1.20057</td>\n      <td>1.02702</td>\n      <td>1.56205</td>\n      <td>1.61993</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1.39209</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>160.02</td>\n      <td>125</td>\n      <td>23.8</td>\n      <td>18.8</td>\n      <td>22.6</td>\n      <td>17.2</td>\n      <td>96.8</td>\n      <td>38</td>\n      <td>0.6</td>\n      <td>0.6</td>\n      <td>77.8</td>\n      <td>22.8</td>\n      <td>0.2</td>\n      <td>18.4</td>\n      <td>14.2</td>\n      <td>1.4</td>\n      <td>0.4</td>\n      <td>120</td>\n      <td>55.8</td>\n      <td>0.474</td>\n      <td>71.2957</td>\n      <td>31.3742</td>\n      <td>0.464392</td>\n      <td>0</td>\n      <td>0.537707</td>\n      <td>7.6</td>\n      <td>3.6</td>\n      <td>0.414</td>\n      <td>168.6</td>\n      <td>97.2</td>\n      <td>93.7534</td>\n      <td>51.4419</td>\n      <td>19.8</td>\n      <td>16</td>\n      <td>20.2</td>\n      <td>15.6</td>\n      <td>70</td>\n      <td>21.4</td>\n      <td>5.8</td>\n      <td>3.8</td>\n      <td>65.6</td>\n      <td>17.6</td>\n      <td>0</td>\n      <td>10.6</td>\n      <td>7.2</td>\n      <td>3.8</td>\n      <td>0.4</td>\n      <td>96</td>\n      <td>40.8</td>\n      <td>0.422</td>\n      <td>65.9586</td>\n      <td>27.0057</td>\n      <td>0.414369</td>\n      <td>0.8</td>\n      <td>0.453785</td>\n      <td>8.4</td>\n      <td>3</td>\n      <td>0.21</td>\n      <td>161.6</td>\n      <td>98.2</td>\n      <td>86.3832</td>\n      <td>45.2787</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>True</td>\n      <td>5</td>\n      <td>2</td>\n      <td>2</td>\n      <td>5</td>\n      <td>162.56</td>\n      <td>4</td>\n      <td>125</td>\n      <td>25</td>\n      <td>15.6667</td>\n      <td>5.66667</td>\n      <td>5.66667</td>\n      <td>94.6667</td>\n      <td>36</td>\n      <td>3.66667</td>\n      <td>2.33333</td>\n      <td>59</td>\n      <td>18</td>\n      <td>0.333333</td>\n      <td>20</td>\n      <td>10.3333</td>\n      <td>3</td>\n      <td>0</td>\n      <td>104</td>\n      <td>44</td>\n      <td>0.403333</td>\n      <td>0.333333</td>\n      <td>9.33333</td>\n      <td>2</td>\n      <td>0.163333</td>\n      <td>119.667</td>\n      <td>57.3333</td>\n      <td>8.33333</td>\n      <td>7.33333</td>\n      <td>4</td>\n      <td>2.33333</td>\n      <td>54.6667</td>\n      <td>22.6667</td>\n      <td>1</td>\n      <td>0.666667</td>\n      <td>48.3333</td>\n      <td>16.3333</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>59.6667</td>\n      <td>25.6667</td>\n      <td>0.423333</td>\n      <td>0</td>\n      <td>0.666667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>64</td>\n      <td>29</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>8</td>\n      <td>703.667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>True</td>\n      <td>17</td>\n      <td>1020</td>\n      <td>1</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0.984471</td>\n      <td>1.01529</td>\n      <td>1</td>\n      <td>0.953846</td>\n      <td>1.188</td>\n      <td>3.54</td>\n      <td>2.73</td>\n      <td>1.0223</td>\n      <td>1.05405</td>\n      <td>0.342857</td>\n      <td>0.48</td>\n      <td>1.31333</td>\n      <td>1.25263</td>\n      <td>0.9</td>\n      <td>0.92381</td>\n      <td>1.34118</td>\n      <td>0.6</td>\n      <td>1.4</td>\n      <td>1.15238</td>\n      <td>1.26222</td>\n      <td>1.05036</td>\n      <td>1.12131</td>\n      <td>0.832258</td>\n      <td>1.53333</td>\n      <td>1.21547</td>\n      <td>1.40552</td>\n      <td>1.68343</td>\n      <td>2.22857</td>\n      <td>2.04</td>\n      <td>4.24</td>\n      <td>4.98</td>\n      <td>1.27545</td>\n      <td>0.946479</td>\n      <td>3.4</td>\n      <td>2.88</td>\n      <td>1.35</td>\n      <td>1.07308</td>\n      <td>1</td>\n      <td>2.9</td>\n      <td>2.73333</td>\n      <td>4.8</td>\n      <td>1.4</td>\n      <td>1.5989</td>\n      <td>1.5675</td>\n      <td>0.999063</td>\n      <td>1.10828</td>\n      <td>5.64</td>\n      <td>4</td>\n      <td>1.21</td>\n      <td>2.50154</td>\n      <td>3.30667</td>\n      <td>1</td>\n      <td>0.5</td>\n      <td>1</td>\n      <td>0.75</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1.44891</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1.33333</td>\n      <td>0.5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>165.1</td>\n      <td>115</td>\n      <td>16.3333</td>\n      <td>8.33333</td>\n      <td>7</td>\n      <td>5</td>\n      <td>78.6667</td>\n      <td>20.6667</td>\n      <td>6.66667</td>\n      <td>4.66667</td>\n      <td>70.6667</td>\n      <td>18</td>\n      <td>0</td>\n      <td>5.33333</td>\n      <td>4</td>\n      <td>2.66667</td>\n      <td>0.333333</td>\n      <td>69.894</td>\n      <td>30.4553</td>\n      <td>0.455581</td>\n      <td>92.3333</td>\n      <td>30.3333</td>\n      <td>0.32</td>\n      <td>0.550644</td>\n      <td>0</td>\n      <td>8.33333</td>\n      <td>1.33333</td>\n      <td>0.166667</td>\n      <td>92.116</td>\n      <td>50.2807</td>\n      <td>116.333</td>\n      <td>50</td>\n      <td>19.3333</td>\n      <td>15.3333</td>\n      <td>33.3333</td>\n      <td>21.3333</td>\n      <td>129</td>\n      <td>72</td>\n      <td>8</td>\n      <td>6</td>\n      <td>138.333</td>\n      <td>71.6667</td>\n      <td>0</td>\n      <td>12.6667</td>\n      <td>12.3333</td>\n      <td>0.666667</td>\n      <td>0</td>\n      <td>66.5621</td>\n      <td>27.7264</td>\n      <td>0.426225</td>\n      <td>170.333</td>\n      <td>99.3333</td>\n      <td>0.57</td>\n      <td>0.470368</td>\n      <td>0</td>\n      <td>0.666667</td>\n      <td>0.333333</td>\n      <td>0.166667</td>\n      <td>86.8973</td>\n      <td>45.9246</td>\n      <td>192.333</td>\n      <td>118.667</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>23</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>152.4</td>\n      <td>1</td>\n      <td>115</td>\n      <td>5.5</td>\n      <td>2.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>132</td>\n      <td>31</td>\n      <td>0</td>\n      <td>0</td>\n      <td>117</td>\n      <td>22</td>\n      <td>0</td>\n      <td>9.5</td>\n      <td>6.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>132</td>\n      <td>31</td>\n      <td>0.215</td>\n      <td>0</td>\n      <td>1.5</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>132</td>\n      <td>31</td>\n      <td>8</td>\n      <td>3.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>177</td>\n      <td>24</td>\n      <td>0</td>\n      <td>0</td>\n      <td>165.5</td>\n      <td>18.5</td>\n      <td>0</td>\n      <td>3.5</td>\n      <td>2</td>\n      <td>0.5</td>\n      <td>0</td>\n      <td>177</td>\n      <td>24</td>\n      <td>0.18</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>177.5</td>\n      <td>24.5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>6</td>\n      <td>900</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>False</td>\n      <td>8</td>\n      <td>746</td>\n      <td>1</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.08279</td>\n      <td>1.11591</td>\n      <td>1</td>\n      <td>2.66667</td>\n      <td>2.66667</td>\n      <td>8</td>\n      <td>6</td>\n      <td>0.598997</td>\n      <td>0.677083</td>\n      <td>7.66667</td>\n      <td>5.66667</td>\n      <td>0.607345</td>\n      <td>0.826087</td>\n      <td>1</td>\n      <td>0.603175</td>\n      <td>0.666667</td>\n      <td>3.66667</td>\n      <td>1.33333</td>\n      <td>1.43033</td>\n      <td>1.39269</td>\n      <td>1.00165</td>\n      <td>1</td>\n      <td>3.73333</td>\n      <td>1.55556</td>\n      <td>0.777778</td>\n      <td>1.40671</td>\n      <td>1.4523</td>\n      <td>2.25926</td>\n      <td>3.62963</td>\n      <td>34.3333</td>\n      <td>22.3333</td>\n      <td>0.730337</td>\n      <td>2.92</td>\n      <td>9</td>\n      <td>7</td>\n      <td>0.836837</td>\n      <td>3.7265</td>\n      <td>1</td>\n      <td>3.03704</td>\n      <td>4.44444</td>\n      <td>1.11111</td>\n      <td>1</td>\n      <td>1.56157</td>\n      <td>1.51032</td>\n      <td>1.01581</td>\n      <td>1</td>\n      <td>1.11111</td>\n      <td>1.33333</td>\n      <td>1.16667</td>\n      <td>1.56205</td>\n      <td>1.61993</td>\n      <td>3</td>\n      <td>0.5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1.5</td>\n      <td>1.28571</td>\n      <td>0.829079</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>182.88</td>\n      <td>170</td>\n      <td>8.68893</td>\n      <td>6.05741</td>\n      <td>8.08863</td>\n      <td>5.44475</td>\n      <td>53.3634</td>\n      <td>19.4233</td>\n      <td>9.19593</td>\n      <td>6.08155</td>\n      <td>55.8417</td>\n      <td>20.0612</td>\n      <td>0.255765</td>\n      <td>6.11738</td>\n      <td>4.83103</td>\n      <td>1.34854</td>\n      <td>0.160135</td>\n      <td>69.894</td>\n      <td>30.4553</td>\n      <td>0.455581</td>\n      <td>71.2957</td>\n      <td>31.3742</td>\n      <td>0.464392</td>\n      <td>0.550644</td>\n      <td>0.537707</td>\n      <td>2.89827</td>\n      <td>1.2467</td>\n      <td>0.323823</td>\n      <td>92.116</td>\n      <td>50.2807</td>\n      <td>93.7534</td>\n      <td>51.4419</td>\n      <td>8.23462</td>\n      <td>5.55821</td>\n      <td>7.36715</td>\n      <td>4.84363</td>\n      <td>52.061</td>\n      <td>18.0976</td>\n      <td>6.80935</td>\n      <td>4.39742</td>\n      <td>52.023</td>\n      <td>17.1109</td>\n      <td>0.155104</td>\n      <td>5.97988</td>\n      <td>4.6696</td>\n      <td>1.04796</td>\n      <td>0.157535</td>\n      <td>66.5621</td>\n      <td>27.7264</td>\n      <td>0.426225</td>\n      <td>65.9586</td>\n      <td>27.0057</td>\n      <td>0.414369</td>\n      <td>0.470368</td>\n      <td>0.453785</td>\n      <td>2.86408</td>\n      <td>1.0574</td>\n      <td>0.266356</td>\n      <td>86.8973</td>\n      <td>45.9246</td>\n      <td>86.3832</td>\n      <td>45.2787</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>94</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>185.42</td>\n      <td>1</td>\n      <td>185</td>\n      <td>3.33333</td>\n      <td>2.66667</td>\n      <td>3</td>\n      <td>2</td>\n      <td>6.66667</td>\n      <td>3</td>\n      <td>4</td>\n      <td>3.33333</td>\n      <td>7.66667</td>\n      <td>3.66667</td>\n      <td>0</td>\n      <td>2.66667</td>\n      <td>2</td>\n      <td>2.66667</td>\n      <td>0</td>\n      <td>13.6667</td>\n      <td>8.33333</td>\n      <td>0.626667</td>\n      <td>1</td>\n      <td>5.66667</td>\n      <td>2.66667</td>\n      <td>0.306667</td>\n      <td>29</td>\n      <td>23</td>\n      <td>6.66667</td>\n      <td>5</td>\n      <td>6.66667</td>\n      <td>4.66667</td>\n      <td>8.33333</td>\n      <td>3</td>\n      <td>7.66667</td>\n      <td>6</td>\n      <td>13.3333</td>\n      <td>7</td>\n      <td>0</td>\n      <td>2.66667</td>\n      <td>1.66667</td>\n      <td>0.333333</td>\n      <td>1</td>\n      <td>22.6667</td>\n      <td>13.6667</td>\n      <td>0.59</td>\n      <td>1.33333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>75</td>\n      <td>65.3333</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>4</td>\n      <td>638.333</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>False</td>\n      <td>0</td>\n      <td>596.98</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.986375</td>\n      <td>1.00095</td>\n      <td>0.919355</td>\n      <td>1.60701</td>\n      <td>1.55544</td>\n      <td>1.85307</td>\n      <td>1.72106</td>\n      <td>1.87933</td>\n      <td>1.75692</td>\n      <td>2.11033</td>\n      <td>1.85015</td>\n      <td>1.67306</td>\n      <td>1.55684</td>\n      <td>1.05014</td>\n      <td>1.7786</td>\n      <td>1.67947</td>\n      <td>1.27741</td>\n      <td>1.03717</td>\n      <td>1.43033</td>\n      <td>1.39269</td>\n      <td>1.00165</td>\n      <td>1.12131</td>\n      <td>1.4646</td>\n      <td>1.23351</td>\n      <td>1.02838</td>\n      <td>1.40671</td>\n      <td>1.4523</td>\n      <td>1.5148</td>\n      <td>1.46602</td>\n      <td>1.79824</td>\n      <td>1.67377</td>\n      <td>1.82982</td>\n      <td>1.71755</td>\n      <td>2.23717</td>\n      <td>1.89585</td>\n      <td>1.67112</td>\n      <td>1.60314</td>\n      <td>1.02981</td>\n      <td>1.58847</td>\n      <td>1.52051</td>\n      <td>1.2442</td>\n      <td>1.03812</td>\n      <td>1.56157</td>\n      <td>1.51032</td>\n      <td>1.01581</td>\n      <td>1.10828</td>\n      <td>1.40841</td>\n      <td>1.20057</td>\n      <td>1.02702</td>\n      <td>1.56205</td>\n      <td>1.61993</td>\n      <td>1</td>\n      <td>0.5</td>\n      <td>1</td>\n      <td>0.5</td>\n      <td>0.333333</td>\n      <td>0.2</td>\n      <td>1.39209</td>\n      <td>0.5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.5</td>\n      <td>1</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>2567</td>\n      <td>177.8</td>\n      <td>155</td>\n      <td>14.2</td>\n      <td>8.2</td>\n      <td>7.4</td>\n      <td>4.6</td>\n      <td>106</td>\n      <td>43</td>\n      <td>0.6</td>\n      <td>0.6</td>\n      <td>89.6</td>\n      <td>31.8</td>\n      <td>0</td>\n      <td>10.2</td>\n      <td>8.2</td>\n      <td>0.2</td>\n      <td>0</td>\n      <td>114</td>\n      <td>48.2</td>\n      <td>0.426</td>\n      <td>71.2957</td>\n      <td>31.3742</td>\n      <td>0.464392</td>\n      <td>0</td>\n      <td>0.537707</td>\n      <td>2.8</td>\n      <td>1</td>\n      <td>0.3</td>\n      <td>121.4</td>\n      <td>54.6</td>\n      <td>93.7534</td>\n      <td>51.4419</td>\n      <td>15.6</td>\n      <td>7</td>\n      <td>4</td>\n      <td>2</td>\n      <td>109.6</td>\n      <td>28.6</td>\n      <td>2.8</td>\n      <td>2.6</td>\n      <td>90.4</td>\n      <td>17.6</td>\n      <td>0</td>\n      <td>10.4</td>\n      <td>8.6</td>\n      <td>0.6</td>\n      <td>0</td>\n      <td>116.4</td>\n      <td>33.2</td>\n      <td>0.312</td>\n      <td>65.9586</td>\n      <td>27.0057</td>\n      <td>0.414369</td>\n      <td>0.6</td>\n      <td>0.453785</td>\n      <td>4.6</td>\n      <td>1.2</td>\n      <td>0.128</td>\n      <td>124.2</td>\n      <td>41</td>\n      <td>86.3832</td>\n      <td>45.2787</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n      <td>27</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n      <td>172.72</td>\n      <td>1</td>\n      <td>155</td>\n      <td>18.96</td>\n      <td>12.4</td>\n      <td>9.8</td>\n      <td>6.88</td>\n      <td>97.84</td>\n      <td>36.12</td>\n      <td>2.04</td>\n      <td>1.08</td>\n      <td>80.76</td>\n      <td>23.48</td>\n      <td>0.24</td>\n      <td>9.96</td>\n      <td>8.2</td>\n      <td>0.12</td>\n      <td>0.04</td>\n      <td>109.68</td>\n      <td>44.08</td>\n      <td>0.4144</td>\n      <td>0</td>\n      <td>1.64</td>\n      <td>0.6</td>\n      <td>0.32</td>\n      <td>120.68</td>\n      <td>53.88</td>\n      <td>14.36</td>\n      <td>9.56</td>\n      <td>12.24</td>\n      <td>8.08</td>\n      <td>114.72</td>\n      <td>38.68</td>\n      <td>2.76</td>\n      <td>1.76</td>\n      <td>104.76</td>\n      <td>30.88</td>\n      <td>0.28</td>\n      <td>10.6</td>\n      <td>8.08</td>\n      <td>0.28</td>\n      <td>0.04</td>\n      <td>129.72</td>\n      <td>48.52</td>\n      <td>0.372</td>\n      <td>0.08</td>\n      <td>3.32</td>\n      <td>0.8</td>\n      <td>0.1924</td>\n      <td>140</td>\n      <td>57.6</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>13</td>\n      <td>65</td>\n      <td>719.72</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>5</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>12</td>\n      <td>False</td>\n      <td>15</td>\n      <td>900</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1.02924</td>\n      <td>1.05764</td>\n      <td>1</td>\n      <td>0.761523</td>\n      <td>0.686567</td>\n      <td>0.777778</td>\n      <td>0.71066</td>\n      <td>1.08256</td>\n      <td>1.18534</td>\n      <td>0.526316</td>\n      <td>0.769231</td>\n      <td>1.10812</td>\n      <td>1.33987</td>\n      <td>0.806452</td>\n      <td>1.0219</td>\n      <td>1</td>\n      <td>1.07143</td>\n      <td>0.961538</td>\n      <td>1.03903</td>\n      <td>1.09139</td>\n      <td>1.0082</td>\n      <td>1.12131</td>\n      <td>1.43939</td>\n      <td>1.25</td>\n      <td>0.984848</td>\n      <td>1.00592</td>\n      <td>1.01312</td>\n      <td>1.08073</td>\n      <td>0.757576</td>\n      <td>0.377644</td>\n      <td>0.330396</td>\n      <td>0.955755</td>\n      <td>0.745968</td>\n      <td>1.01064</td>\n      <td>1.30435</td>\n      <td>0.864221</td>\n      <td>0.583438</td>\n      <td>0.78125</td>\n      <td>0.982759</td>\n      <td>1.05727</td>\n      <td>1.25</td>\n      <td>0.961538</td>\n      <td>0.898103</td>\n      <td>0.69063</td>\n      <td>0.956268</td>\n      <td>1.10828</td>\n      <td>1.2963</td>\n      <td>1.22222</td>\n      <td>0.945991</td>\n      <td>0.887943</td>\n      <td>0.716724</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.5</td>\n      <td>0.285714</td>\n      <td>0.242424</td>\n      <td>1.25014</td>\n      <td>0.5</td>\n      <td>1</td>\n      <td>0.666667</td>\n      <td>0.333333</td>\n      <td>0.2</td>\n      <td>1</td>\n      <td>0.5</td>\n      <td>0.230769</td>\n    </tr>\n    <tr>\n      <td>2568</td>\n      <td>180.34</td>\n      <td>155</td>\n      <td>18.75</td>\n      <td>11.25</td>\n      <td>1.4375</td>\n      <td>0.875</td>\n      <td>98.1875</td>\n      <td>39.9375</td>\n      <td>2.5625</td>\n      <td>1.5</td>\n      <td>65.5</td>\n      <td>17.25</td>\n      <td>0.5</td>\n      <td>17.9375</td>\n      <td>13.8125</td>\n      <td>0.125</td>\n      <td>0</td>\n      <td>69.894</td>\n      <td>30.4553</td>\n      <td>0.455581</td>\n      <td>102.188</td>\n      <td>42.3125</td>\n      <td>0.45375</td>\n      <td>0.550644</td>\n      <td>0.0625</td>\n      <td>0.8125</td>\n      <td>0.4375</td>\n      <td>0.22875</td>\n      <td>92.116</td>\n      <td>50.2807</td>\n      <td>103.625</td>\n      <td>43.375</td>\n      <td>9.6875</td>\n      <td>5.125</td>\n      <td>1.1875</td>\n      <td>0.625</td>\n      <td>95.0625</td>\n      <td>31.8125</td>\n      <td>6.25</td>\n      <td>3.8125</td>\n      <td>84.9375</td>\n      <td>25.0625</td>\n      <td>0.125</td>\n      <td>7.875</td>\n      <td>6.0625</td>\n      <td>0.375</td>\n      <td>0</td>\n      <td>66.5621</td>\n      <td>27.7264</td>\n      <td>0.426225</td>\n      <td>102.5</td>\n      <td>36.25</td>\n      <td>0.378125</td>\n      <td>0.470368</td>\n      <td>0.3125</td>\n      <td>3.1875</td>\n      <td>0.4375</td>\n      <td>0.214375</td>\n      <td>86.8973</td>\n      <td>45.9246</td>\n      <td>110.562</td>\n      <td>43</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>True</td>\n      <td>81</td>\n      <td>4</td>\n      <td>4</td>\n      <td>3</td>\n      <td>177.8</td>\n      <td>4</td>\n      <td>155</td>\n      <td>15.1</td>\n      <td>9.8</td>\n      <td>13.2</td>\n      <td>8.6</td>\n      <td>51.5</td>\n      <td>20.3</td>\n      <td>7.2</td>\n      <td>4.9</td>\n      <td>50.2</td>\n      <td>18.5</td>\n      <td>0.3</td>\n      <td>6.6</td>\n      <td>5.5</td>\n      <td>2</td>\n      <td>0</td>\n      <td>71.9</td>\n      <td>33.8</td>\n      <td>0.48</td>\n      <td>0.5</td>\n      <td>3.3</td>\n      <td>0.9</td>\n      <td>0.291</td>\n      <td>80.8</td>\n      <td>41.5</td>\n      <td>15.2</td>\n      <td>8.6</td>\n      <td>7.3</td>\n      <td>4.8</td>\n      <td>57.2</td>\n      <td>20</td>\n      <td>2.4</td>\n      <td>1.9</td>\n      <td>47.5</td>\n      <td>14.4</td>\n      <td>0.2</td>\n      <td>4.2</td>\n      <td>3.7</td>\n      <td>0.8</td>\n      <td>0.1</td>\n      <td>66.9</td>\n      <td>26.7</td>\n      <td>0.411</td>\n      <td>0.6</td>\n      <td>1.6</td>\n      <td>0.4</td>\n      <td>0.225</td>\n      <td>82.1</td>\n      <td>38.9</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2</td>\n      <td>21</td>\n      <td>557.7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>8</td>\n      <td>False</td>\n      <td>38</td>\n      <td>635.5</td>\n      <td>0</td>\n      <td>8</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12</td>\n      <td>1.01421</td>\n      <td>1.04144</td>\n      <td>1</td>\n      <td>1.22671</td>\n      <td>1.13426</td>\n      <td>0.171655</td>\n      <td>0.195312</td>\n      <td>1.88929</td>\n      <td>1.92195</td>\n      <td>0.434451</td>\n      <td>0.423729</td>\n      <td>1.29883</td>\n      <td>0.935897</td>\n      <td>1.15385</td>\n      <td>2.49178</td>\n      <td>2.27885</td>\n      <td>0.375</td>\n      <td>1</td>\n      <td>1.43033</td>\n      <td>1.39269</td>\n      <td>1.00165</td>\n      <td>0.708333</td>\n      <td>0.421512</td>\n      <td>0.756579</td>\n      <td>0.951782</td>\n      <td>1.40671</td>\n      <td>1.4523</td>\n      <td>0.659722</td>\n      <td>0.638021</td>\n      <td>0.263554</td>\n      <td>0.280172</td>\n      <td>1.65056</td>\n      <td>1.5625</td>\n      <td>2.13235</td>\n      <td>1.65948</td>\n      <td>1.77191</td>\n      <td>1.69237</td>\n      <td>0.9375</td>\n      <td>1.70673</td>\n      <td>1.50266</td>\n      <td>0.763889</td>\n      <td>0.909091</td>\n      <td>1.56157</td>\n      <td>1.51032</td>\n      <td>1.01581</td>\n      <td>0.820312</td>\n      <td>1.61058</td>\n      <td>1.02679</td>\n      <td>0.991327</td>\n      <td>1.56205</td>\n      <td>1.61993</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.833333</td>\n      <td>1.66667</td>\n      <td>1.77273</td>\n      <td>1.13925</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1.5</td>\n      <td>3</td>\n      <td>0.25</td>\n      <td>1</td>\n      <td>1.44444</td>\n    </tr>\n    <tr>\n      <td>2569</td>\n      <td>187.96</td>\n      <td>185</td>\n      <td>7.6</td>\n      <td>6.9</td>\n      <td>8</td>\n      <td>7.1</td>\n      <td>22.5</td>\n      <td>14.9</td>\n      <td>10</td>\n      <td>8.2</td>\n      <td>25</td>\n      <td>16</td>\n      <td>1.1</td>\n      <td>7.9</td>\n      <td>7.3</td>\n      <td>0.2</td>\n      <td>0.1</td>\n      <td>69.894</td>\n      <td>30.4553</td>\n      <td>0.455581</td>\n      <td>40.5</td>\n      <td>30.2</td>\n      <td>0.77</td>\n      <td>0.550644</td>\n      <td>0.3</td>\n      <td>0.3</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>92.116</td>\n      <td>50.2807</td>\n      <td>50</td>\n      <td>38.4</td>\n      <td>2.1</td>\n      <td>1.4</td>\n      <td>2.7</td>\n      <td>1.5</td>\n      <td>24</td>\n      <td>6</td>\n      <td>3.5</td>\n      <td>1.8</td>\n      <td>22.5</td>\n      <td>3.9</td>\n      <td>0</td>\n      <td>5.6</td>\n      <td>4</td>\n      <td>0.6</td>\n      <td>0</td>\n      <td>66.5621</td>\n      <td>27.7264</td>\n      <td>0.426225</td>\n      <td>30.2</td>\n      <td>9.3</td>\n      <td>0.278</td>\n      <td>0.470368</td>\n      <td>0</td>\n      <td>2.9</td>\n      <td>0.7</td>\n      <td>0.2</td>\n      <td>86.8973</td>\n      <td>45.9246</td>\n      <td>37.3</td>\n      <td>15.5</td>\n      <td>0</td>\n      <td>10</td>\n      <td>0</td>\n      <td>True</td>\n      <td>103</td>\n      <td>10</td>\n      <td>0</td>\n      <td>5</td>\n      <td>185.42</td>\n      <td>4</td>\n      <td>170</td>\n      <td>2.85714</td>\n      <td>1.85714</td>\n      <td>2.71429</td>\n      <td>1.57143</td>\n      <td>13.2857</td>\n      <td>4.42857</td>\n      <td>10.1429</td>\n      <td>6.28571</td>\n      <td>22.2857</td>\n      <td>9.71429</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.714286</td>\n      <td>2.57143</td>\n      <td>0.428571</td>\n      <td>26.1429</td>\n      <td>12.2857</td>\n      <td>0.491429</td>\n      <td>1.28571</td>\n      <td>3.85714</td>\n      <td>1.57143</td>\n      <td>0.42</td>\n      <td>53.5714</td>\n      <td>34.5714</td>\n      <td>3.42857</td>\n      <td>1.28571</td>\n      <td>1.57143</td>\n      <td>1.14286</td>\n      <td>16.5714</td>\n      <td>3.57143</td>\n      <td>6.14286</td>\n      <td>2.85714</td>\n      <td>18.1429</td>\n      <td>4.14286</td>\n      <td>0.142857</td>\n      <td>2.71429</td>\n      <td>2.14286</td>\n      <td>0.285714</td>\n      <td>0.285714</td>\n      <td>24.2857</td>\n      <td>7.57143</td>\n      <td>0.445714</td>\n      <td>0.571429</td>\n      <td>0.428571</td>\n      <td>0.142857</td>\n      <td>0.142857</td>\n      <td>36.7143</td>\n      <td>19.7143</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5</td>\n      <td>1</td>\n      <td>12</td>\n      <td>368.857</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0</td>\n      <td>6</td>\n      <td>True</td>\n      <td>19</td>\n      <td>431.1</td>\n      <td>6</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>7</td>\n      <td>2</td>\n      <td>0</td>\n      <td>10</td>\n      <td>1.01363</td>\n      <td>1.06907</td>\n      <td>1.08772</td>\n      <td>2.22963</td>\n      <td>2.765</td>\n      <td>2.42308</td>\n      <td>3.15</td>\n      <td>1.645</td>\n      <td>2.92895</td>\n      <td>0.987179</td>\n      <td>1.26275</td>\n      <td>1.11656</td>\n      <td>1.58667</td>\n      <td>2.1</td>\n      <td>4.45</td>\n      <td>4.84167</td>\n      <td>0.336</td>\n      <td>0.77</td>\n      <td>1.43033</td>\n      <td>1.39269</td>\n      <td>1.00165</td>\n      <td>0.56875</td>\n      <td>0.267647</td>\n      <td>0.466667</td>\n      <td>0.84507</td>\n      <td>1.40671</td>\n      <td>1.4523</td>\n      <td>0.7</td>\n      <td>1.05</td>\n      <td>1.43889</td>\n      <td>1.16667</td>\n      <td>1.42276</td>\n      <td>1.53125</td>\n      <td>0.63</td>\n      <td>0.725926</td>\n      <td>1.22761</td>\n      <td>0.952778</td>\n      <td>0.875</td>\n      <td>1.77692</td>\n      <td>1.59091</td>\n      <td>1.24444</td>\n      <td>0.777778</td>\n      <td>1.56157</td>\n      <td>1.51032</td>\n      <td>1.01581</td>\n      <td>0.636364</td>\n      <td>2.73</td>\n      <td>1.4875</td>\n      <td>1.05</td>\n      <td>1.56205</td>\n      <td>1.61993</td>\n      <td>1</td>\n      <td>5.5</td>\n      <td>1</td>\n      <td>1.83333</td>\n      <td>0.5</td>\n      <td>1.53846</td>\n      <td>1.16829</td>\n      <td>7</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>8</td>\n      <td>0.5</td>\n      <td>1</td>\n      <td>1.57143</td>\n    </tr>\n    <tr>\n      <td>2570</td>\n      <td>170.18</td>\n      <td>135</td>\n      <td>11.6364</td>\n      <td>8.90909</td>\n      <td>11</td>\n      <td>6.45455</td>\n      <td>94.7273</td>\n      <td>32.9091</td>\n      <td>14.4545</td>\n      <td>8.45455</td>\n      <td>102.727</td>\n      <td>35</td>\n      <td>0.272727</td>\n      <td>5.81818</td>\n      <td>3.90909</td>\n      <td>1</td>\n      <td>0</td>\n      <td>69.894</td>\n      <td>30.4553</td>\n      <td>0.455581</td>\n      <td>120.182</td>\n      <td>47.8182</td>\n      <td>0.37</td>\n      <td>0.550644</td>\n      <td>0.0909091</td>\n      <td>2.81818</td>\n      <td>1.45455</td>\n      <td>0.388182</td>\n      <td>92.116</td>\n      <td>50.2807</td>\n      <td>148.455</td>\n      <td>71.6364</td>\n      <td>14.7273</td>\n      <td>11.9091</td>\n      <td>12.2727</td>\n      <td>10</td>\n      <td>81.4545</td>\n      <td>28.2727</td>\n      <td>3</td>\n      <td>2.54545</td>\n      <td>78.4545</td>\n      <td>25.7273</td>\n      <td>0</td>\n      <td>3.54545</td>\n      <td>3.18182</td>\n      <td>0.727273</td>\n      <td>0.0909091</td>\n      <td>66.5621</td>\n      <td>27.7264</td>\n      <td>0.426225</td>\n      <td>96.7273</td>\n      <td>40.8182</td>\n      <td>0.470909</td>\n      <td>0.470368</td>\n      <td>0.272727</td>\n      <td>5</td>\n      <td>1.54545</td>\n      <td>0.336364</td>\n      <td>86.8973</td>\n      <td>45.9246</td>\n      <td>122.091</td>\n      <td>63.7273</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n      <td>48</td>\n      <td>5</td>\n      <td>4</td>\n      <td>3</td>\n      <td>185.42</td>\n      <td>1</td>\n      <td>135</td>\n      <td>13.5833</td>\n      <td>8.5</td>\n      <td>10.25</td>\n      <td>5.91667</td>\n      <td>68.25</td>\n      <td>19.9167</td>\n      <td>13.5833</td>\n      <td>9.5</td>\n      <td>72.3333</td>\n      <td>22.5833</td>\n      <td>0.166667</td>\n      <td>6.16667</td>\n      <td>4.25</td>\n      <td>0.25</td>\n      <td>0.0833333</td>\n      <td>92.0833</td>\n      <td>35.3333</td>\n      <td>0.361667</td>\n      <td>0.0833333</td>\n      <td>1.16667</td>\n      <td>0.666667</td>\n      <td>0.325833</td>\n      <td>129.333</td>\n      <td>69.75</td>\n      <td>5.33333</td>\n      <td>4</td>\n      <td>5.08333</td>\n      <td>3.16667</td>\n      <td>35.8333</td>\n      <td>14.4167</td>\n      <td>7.75</td>\n      <td>4.16667</td>\n      <td>37.0833</td>\n      <td>12.25</td>\n      <td>0.5</td>\n      <td>6.25</td>\n      <td>5.5</td>\n      <td>2</td>\n      <td>0.0833333</td>\n      <td>48.6667</td>\n      <td>21.75</td>\n      <td>0.436667</td>\n      <td>0.333333</td>\n      <td>4.66667</td>\n      <td>1.91667</td>\n      <td>0.3325</td>\n      <td>68.8333</td>\n      <td>40.4167</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>7</td>\n      <td>28</td>\n      <td>602.083</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>False</td>\n      <td>30</td>\n      <td>778.364</td>\n      <td>0</td>\n      <td>9</td>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>0.918249</td>\n      <td>0.944747</td>\n      <td>1</td>\n      <td>0.866494</td>\n      <td>1.04306</td>\n      <td>1.06667</td>\n      <td>1.07777</td>\n      <td>1.38234</td>\n      <td>1.62115</td>\n      <td>1.05974</td>\n      <td>0.900433</td>\n      <td>1.41446</td>\n      <td>1.5265</td>\n      <td>1.09091</td>\n      <td>0.951374</td>\n      <td>0.935065</td>\n      <td>1.6</td>\n      <td>0.923077</td>\n      <td>1.43033</td>\n      <td>1.39269</td>\n      <td>1.00165</td>\n      <td>1.00699</td>\n      <td>1.76224</td>\n      <td>1.47273</td>\n      <td>1.04703</td>\n      <td>1.40671</td>\n      <td>1.4523</td>\n      <td>2.48325</td>\n      <td>2.58182</td>\n      <td>2.18182</td>\n      <td>2.64</td>\n      <td>2.23858</td>\n      <td>1.89877</td>\n      <td>0.457143</td>\n      <td>0.686217</td>\n      <td>2.08633</td>\n      <td>2.01715</td>\n      <td>0.666667</td>\n      <td>0.626959</td>\n      <td>0.643357</td>\n      <td>0.575758</td>\n      <td>1.00699</td>\n      <td>1.56157</td>\n      <td>1.51032</td>\n      <td>1.01581</td>\n      <td>0.954545</td>\n      <td>1.05882</td>\n      <td>0.872727</td>\n      <td>1.0029</td>\n      <td>1.56205</td>\n      <td>1.61993</td>\n      <td>1.5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.625</td>\n      <td>1.06897</td>\n      <td>1.2923</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1.33333</td>\n      <td>0.666667</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1.33333</td>\n    </tr>\n    <tr>\n      <td>2571</td>\n      <td>182.88</td>\n      <td>170</td>\n      <td>3.33333</td>\n      <td>2.66667</td>\n      <td>6.66667</td>\n      <td>4.33333</td>\n      <td>35</td>\n      <td>13.3333</td>\n      <td>0</td>\n      <td>0</td>\n      <td>32</td>\n      <td>9</td>\n      <td>0</td>\n      <td>6.33333</td>\n      <td>6</td>\n      <td>2</td>\n      <td>0</td>\n      <td>41.6667</td>\n      <td>17.6667</td>\n      <td>0.326667</td>\n      <td>71.2957</td>\n      <td>31.3742</td>\n      <td>0.464392</td>\n      <td>1.33333</td>\n      <td>0.537707</td>\n      <td>4.33333</td>\n      <td>1</td>\n      <td>0.456667</td>\n      <td>46</td>\n      <td>21.3333</td>\n      <td>93.7534</td>\n      <td>51.4419</td>\n      <td>18</td>\n      <td>13.6667</td>\n      <td>9</td>\n      <td>7.66667</td>\n      <td>38</td>\n      <td>16.3333</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>21</td>\n      <td>4.66667</td>\n      <td>0</td>\n      <td>8.33333</td>\n      <td>6</td>\n      <td>1.33333</td>\n      <td>0</td>\n      <td>47.3333</td>\n      <td>24.3333</td>\n      <td>0.39</td>\n      <td>65.9586</td>\n      <td>27.0057</td>\n      <td>0.414369</td>\n      <td>0.666667</td>\n      <td>0.453785</td>\n      <td>1</td>\n      <td>0.666667</td>\n      <td>0.22</td>\n      <td>57.6667</td>\n      <td>34.3333</td>\n      <td>86.3832</td>\n      <td>45.2787</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>False</td>\n      <td>73</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>182.88</td>\n      <td>1</td>\n      <td>155</td>\n      <td>11</td>\n      <td>6</td>\n      <td>3.66667</td>\n      <td>1.66667</td>\n      <td>61.6667</td>\n      <td>20.3333</td>\n      <td>7</td>\n      <td>6</td>\n      <td>56.6667</td>\n      <td>17.6667</td>\n      <td>0</td>\n      <td>4.66667</td>\n      <td>4.33333</td>\n      <td>2.66667</td>\n      <td>0</td>\n      <td>72.3333</td>\n      <td>28</td>\n      <td>0.393333</td>\n      <td>0.333333</td>\n      <td>5.33333</td>\n      <td>1</td>\n      <td>0.213333</td>\n      <td>97.6667</td>\n      <td>50</td>\n      <td>10</td>\n      <td>7</td>\n      <td>14.3333</td>\n      <td>10.3333</td>\n      <td>72.3333</td>\n      <td>23.6667</td>\n      <td>7.33333</td>\n      <td>4.33333</td>\n      <td>72.6667</td>\n      <td>23.3333</td>\n      <td>0.333333</td>\n      <td>11.3333</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>94</td>\n      <td>38.3333</td>\n      <td>0.44</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0.666667</td>\n      <td>0.166667</td>\n      <td>132.667</td>\n      <td>71.3333</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>8</td>\n      <td>794.333</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>False</td>\n      <td>6</td>\n      <td>558</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.973473</td>\n      <td>1.09615</td>\n      <td>0.361111</td>\n      <td>0.52381</td>\n      <td>1.64286</td>\n      <td>2</td>\n      <td>0.574468</td>\n      <td>0.671875</td>\n      <td>0.125</td>\n      <td>0.142857</td>\n      <td>0.572254</td>\n      <td>0.535714</td>\n      <td>1</td>\n      <td>1.29412</td>\n      <td>1.3125</td>\n      <td>0.818182</td>\n      <td>1</td>\n      <td>0.581818</td>\n      <td>0.643678</td>\n      <td>0.952153</td>\n      <td>1.12131</td>\n      <td>0.842105</td>\n      <td>1</td>\n      <td>1.20055</td>\n      <td>0.476351</td>\n      <td>0.437908</td>\n      <td>1.72727</td>\n      <td>1.83333</td>\n      <td>0.652174</td>\n      <td>0.764706</td>\n      <td>0.531818</td>\n      <td>0.702703</td>\n      <td>0.16</td>\n      <td>0.25</td>\n      <td>0.298643</td>\n      <td>0.232877</td>\n      <td>0.75</td>\n      <td>0.756757</td>\n      <td>0.777778</td>\n      <td>2.33333</td>\n      <td>1</td>\n      <td>0.508772</td>\n      <td>0.644068</td>\n      <td>0.965278</td>\n      <td>1.10828</td>\n      <td>0.4</td>\n      <td>1</td>\n      <td>1.04571</td>\n      <td>0.438903</td>\n      <td>0.488479</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1.5</td>\n      <td>0.777778</td>\n      <td>0.70285</td>\n      <td>0.5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.666667</td>\n    </tr>\n  </tbody>\n</table>\n<p>2572 rows  214 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mD7OF4aDqQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "6          1.000000            1.085714               1.000000   \n\n      avg_CLINCH_att_ratio  avg_CLINCH_landed_ratio  avg_DISTANCE_att_ratio  \\\n1201              0.128788                 0.200000                0.363248   \n3124              1.437500                 1.517857                0.812500   \n1709              1.853073                 1.721062                1.879329   \n3974              0.870370                 1.030303                0.184184   \n274               1.853073                 1.721062                1.879329   \n...                    ...                      ...                     ...   \n4451              0.121951                 0.172414                0.200000   \n190              11.937500                 8.750000                3.629167   \n3967              0.416667                 0.400000                7.642857   \n3335              0.977444                 0.811688                0.313242   \n3646              0.341463                 0.500000                0.778993   \n\n      avg_DISTANCE_landed_ratio  avg_GROUND_att_ratio  \\\n1201                   0.537975              1.789474   \n3124                   1.444853              1.052419   \n1709                   1.756918              2.110335   \n3974                   0.196891              1.851852   \n274                    1.756918              2.110335   \n...                         ...                   ...   \n4451                   0.138889              0.512821   \n190                    7.354167              0.140000   \n3967                   8.200000              2.500000   \n3335                   0.331010              0.592021   \n3646                   1.079365              0.152672   \n\n      avg_GROUND_landed_ratio  avg_HEAD_att_ratio  avg_HEAD_landed_ratio  \\\n1201                 2.500000            0.342636               0.745205   \n3124                 0.990741            0.712355               1.059091   \n1709                 1.850151            1.673061               1.556839   \n3974                 2.121212            0.290672               0.485507   \n274                  1.850151            1.673061               1.556839   \n...                       ...                 ...                    ...   \n4451                 0.357143            0.252525               0.106383   \n190                  0.164062            1.484797               1.238971   \n3967                 2.000000            6.250000               4.800000   \n3335                 0.674603            0.513458               0.647482   \n3646                 0.210526            0.423006               0.342105   \n\n      avg_KD_ratio  avg_LEG_att_ratio  avg_LEG_landed_ratio  avg_PASS_ratio  \\\n1201      0.739130           3.731707              3.777778        0.680000   \n3124      1.178571           1.544118              1.372881        1.442308   \n1709      1.050136           1.778599              1.679467        1.277408   \n3974      0.923077           0.203822              0.145985        2.000000   \n274       1.050136           1.778599              1.679467        1.277408   \n...            ...                ...                   ...             ...   \n4451      1.000000           0.500000              0.625000        0.769231   \n190       1.437500           2.416667              2.937500        0.118750   \n3967      1.000000           2.400000              2.500000        2.000000   \n3335      0.793651           0.346320              0.383598        0.966387   \n3646      0.800000           1.639344              1.481481        0.387097   \n\n      avg_REV_ratio  avg_SIG_STR_att_ratio  avg_SIG_STR_landed_ratio  \\\n1201       0.809524               0.496178                  0.869936   \n3124       1.166667               1.430333                  1.392688   \n1709       1.037172               1.430333                  1.392688   \n3974       1.000000               1.430333                  1.392688   \n274        1.037172               1.430333                  1.392688   \n...             ...                    ...                       ...   \n4451       0.833333               0.206897                  0.120482   \n190        0.333333               1.740385                  1.746528   \n3967       1.000000               5.789474                  4.777778   \n3335       0.974026               1.430333                  1.392688   \n3646       0.800000               0.593272                  0.683761   \n\n      avg_SIG_STR_pct_ratio  avg_SUB_ATT_ratio  avg_TD_att_ratio  \\\n1201               1.140155           1.121309          0.612613   \n3124               1.001652           1.363636          0.415179   \n1709               1.001652           1.121309          1.464598   \n3974               1.001652           1.400000          1.473684   \n274                1.001652           1.121309          1.464598   \n...                     ...                ...               ...   \n4451               0.775194           1.121309          0.555556   \n190                1.032118           1.121309          0.625000   \n3967               0.996528           1.121309          1.500000   \n3335               1.001652           0.952381          0.582011   \n3646               1.158187           1.121309          1.000000   \n\n      avg_TD_landed_ratio  avg_TD_pct_ratio  avg_TOTAL_STR_att_ratio  \\\n1201             0.607143          0.920978                 1.981535   \n3124             0.620690          1.082106                 1.406711   \n1709             1.233509          1.028383                 1.406711   \n3974             1.076923          1.130435                 1.406711   \n274              1.233509          1.028383                 1.406711   \n...                   ...               ...                      ...   \n4451             1.000000          1.569859                 0.263158   \n190              0.531250          0.531250                 0.998355   \n3967             1.500000          1.500000                 4.480000   \n3335             0.621118          0.911217                 1.406711   \n3646             0.666667          0.819188                 0.571429   \n\n      avg_TOTAL_STR_landed_ratio  avg_opp_BODY_att_ratio  \\\n1201                    3.921303                0.089474   \n3124                    1.452297                1.384615   \n1709                    1.452297                1.514798   \n3974                    1.452297                0.852459   \n274                     1.452297                1.514798   \n...                          ...                     ...   \n4451                    0.176768                0.769231   \n190                     0.776250                1.919643   \n3967                    3.000000                2.800000   \n3335                    1.452297                0.752896   \n3646                    0.608899                0.280702   \n\n      avg_opp_BODY_landed_ratio  avg_opp_CLINCH_att_ratio  \\\n1201                   0.161905                  0.101190   \n3124                   1.312500                  3.284483   \n1709                   1.466022                  1.798243   \n3974                   0.792453                  1.205479   \n274                    1.466022                  1.798243   \n...                         ...                       ...   \n4451                   0.909091                  0.196078   \n190                    1.197917                  3.875000   \n3967                   3.833333                  2.000000   \n3335                   0.879121                  2.163866   \n3646                   0.285714                  0.705882   \n\n      avg_opp_CLINCH_landed_ratio  avg_opp_DISTANCE_att_ratio  \\\n1201                     0.168317                    0.114724   \n3124                     3.136364                    0.679891   \n1709                     1.673771                    1.829816   \n3974                     1.080000                    0.365385   \n274                      1.673771                    1.829816   \n...                           ...                         ...   \n4451                     0.370370                    0.078125   \n190                      2.750000                    3.312500   \n3967                     4.000000                   13.550000   \n3335                     2.410714                    0.420918   \n3646                     0.727273                    0.743455   \n\n      avg_opp_DISTANCE_landed_ratio  avg_opp_GROUND_att_ratio  \\\n1201                       0.074561                  0.576271   \n3124                       0.883178                  0.696429   \n1709                       1.717552                  2.237167   \n3974                       0.366667                  1.234043   \n274                        1.717552                  2.237167   \n...                             ...                       ...   \n4451                       0.263158                  0.045455   \n190                        3.020833                  0.328125   \n3967                      12.000000                  1.000000   \n3335                       0.435540                  2.410714   \n3646                       0.356436                  0.705882   \n\n      avg_opp_GROUND_landed_ratio  avg_opp_HEAD_att_ratio  \\\n1201                     0.772727                0.120490   \n3124                     0.750000                0.586651   \n1709                     1.895853                1.671125   \n3974                     1.166667                0.455192   \n274                      1.895853                1.671125   \n...                           ...                     ...   \n4451                     0.072464                0.024876   \n190                      0.296875                2.705882   \n3967                     1.000000               20.500000   \n3335                     2.280220                0.843819   \n3646                     0.727273                0.753181   \n\n      avg_opp_HEAD_landed_ratio  avg_opp_KD_ratio  avg_opp_LEG_att_ratio  \\\n1201                   0.085213          0.739130               0.295652   \n3124                   0.706395          1.050000               2.763158   \n1709                   1.603145          1.029811               1.588474   \n3974                   0.524752          0.888889               0.476190   \n274                    1.603145          1.029811               1.588474   \n...                         ...               ...                    ...   \n4451                   0.053191          1.000000               0.454545   \n190                    2.007812          1.062500              12.562500   \n3967                  36.500000          1.000000               1.833333   \n3335                   1.358314          1.142857               0.904762   \n3646                   0.489796          1.000000               1.882353   \n\n      avg_opp_LEG_landed_ratio  avg_opp_PASS_ratio  avg_opp_REV_ratio  \\\n1201                  0.350515            0.708333           0.894737   \n3124                  2.482759            1.022727           1.000000   \n1709                  1.520515            1.244199           1.038117   \n3974                  0.507042            0.750000           1.000000   \n274                   1.520515            1.244199           1.038117   \n...                        ...                 ...                ...   \n4451                  0.500000            0.238095           1.000000   \n190                  10.625000            0.296875           0.500000   \n3967                  2.500000            0.200000           1.000000   \n3335                  1.047619            0.952381           0.974026   \n3646                  1.000000            1.000000           0.727273   \n\n      avg_opp_SIG_STR_att_ratio  avg_opp_SIG_STR_landed_ratio  \\\n1201                   0.111903                      0.089947   \n3124                   1.561574                      1.510316   \n1709                   1.561574                      1.510316   \n3974                   1.561574                      1.510316   \n274                    1.561574                      1.510316   \n...                         ...                           ...   \n4451                   0.046512                      0.095238   \n190                    3.043478                      2.451923   \n3967                  11.708333                     12.750000   \n3335                   1.561574                      1.510316   \n3646                   0.727273                      0.428571   \n\n      avg_opp_SIG_STR_pct_ratio  avg_opp_SUB_ATT_ratio  avg_opp_TD_att_ratio  \\\n1201                   0.897138               1.108284              0.229730   \n3124                   1.015809               0.910714              2.700000   \n1709                   1.015809               1.108284              1.408409   \n3974                   1.015809               1.090909              0.742857   \n274                    1.015809               1.108284              1.408409   \n...                         ...                    ...                   ...   \n4451                   1.422475               1.108284              0.250000   \n190                    0.958604               1.108284              0.468750   \n3967                   1.031496               1.108284              0.666667   \n3335                   1.015809               0.595238              1.038961   \n3646                   0.841901               1.108284              0.615385   \n\n      avg_opp_TD_landed_ratio  avg_opp_TD_pct_ratio  \\\n1201                 0.629630              0.865140   \n3124                 1.187500              1.049268   \n1709                 1.200573              1.027018   \n3974                 1.000000              1.203456   \n274                  1.200573              1.027018   \n...                       ...                   ...   \n4451                 0.357143              0.818331   \n190                  0.359375              0.777289   \n3967                 0.333333              0.500000   \n3335                 1.153846              1.029870   \n3646                 0.666667              1.128881   \n\n      avg_opp_TOTAL_STR_att_ratio  avg_opp_TOTAL_STR_landed_ratio  \\\n1201                     0.215594                        0.286058   \n3124                     1.562048                        1.619928   \n1709                     1.562048                        1.619928   \n3974                     1.562048                        1.619928   \n274                      1.562048                        1.619928   \n...                           ...                             ...   \n4451                     0.043988                        0.068807   \n190                      1.396552                        0.919837   \n3967                     7.447368                        4.727273   \n3335                     1.562048                        1.619928   \n3646                     0.570571                        0.324638   \n\n      current_lose_streak_ratio  current_win_streak_ratio  draw_ratio  \\\n1201                        1.0                  1.000000         1.0   \n3124                        2.0                  0.333333         1.0   \n1709                        1.0                  1.000000         1.0   \n3974                        1.0                  1.000000         1.0   \n274                         1.0                  1.000000         1.0   \n...                         ...                       ...         ...   \n4451                        1.0                  1.000000         1.0   \n190                         2.0                  0.500000         1.0   \n3967                        1.0                  1.000000         1.0   \n3335                        1.0                  0.666667         1.0   \n3646                        1.0                  0.500000         1.0   \n\n      longest_win_streak_ratio  losses_ratio  total_rounds_fought_ratio  \\\n1201                  0.222222      0.125000                   0.081633   \n3124                  1.500000      1.333333                   1.476190   \n1709                  1.000000      1.000000                   1.000000   \n3974                  1.666667      0.400000                   0.291667   \n274                   1.000000      1.000000                   1.000000   \n...                        ...           ...                        ...   \n4451                  0.250000      0.666667                   0.166667   \n190                   2.500000      7.000000                   8.750000   \n3967                  2.000000      1.000000                   2.500000   \n3335                  1.000000      1.500000                   1.105263   \n3646                  0.500000      0.500000                   0.227273   \n\n      total_time_fought(seconds)_ratio  total_title_bouts_ratio  \\\n1201                          1.153475                 0.666667   \n3124                          1.226673                 2.500000   \n1709                          1.392094                 1.000000   \n3974                          0.408085                 1.000000   \n274                           1.392094                 1.000000   \n...                                ...                      ...   \n4451                          0.247200                 0.333333   \n190                           0.640135                 1.000000   \n3967                          2.448718                 1.000000   \n3335                          0.769231                 5.000000   \n3646                          0.542887                 1.000000   \n\n      win_by_Decision_Majority_ratio  win_by_Decision_Split_ratio  \\\n1201                             1.0                     0.250000   \n3124                             1.0                     1.000000   \n1709                             1.0                     1.000000   \n3974                             1.0                     0.333333   \n274                              1.0                     1.000000   \n...                              ...                          ...   \n4451                             1.0                     1.000000   \n190                              1.0                     3.000000   \n3967                             1.0                     1.000000   \n3335                             1.0                     1.000000   \n3646                             1.0                     1.000000   \n\n      win_by_Decision_Unanimous_ratio  win_by_KO/TKO_ratio  \\\n1201                         0.285714             0.500000   \n3124                         1.200000             1.000000   \n1709                         1.000000             1.000000   \n3974                         0.333333             1.666667   \n274                          1.000000             1.000000   \n...                               ...                  ...   \n4451                         1.000000             1.000000   \n190                          3.000000             6.000000   \n3967                         1.000000             1.000000   \n3335                         0.500000             1.142857   \n3646                         0.200000             0.500000   \n\n      win_by_Submission_ratio  win_by_TKO_Doctor_Stoppage_ratio  wins_ratio  \n1201                 1.000000                               1.0    0.181818  \n3124                 2.000000                               1.0    1.250000  \n1709                 1.000000                               1.0    1.000000  \n3974                 1.000000                               1.0    0.714286  \n274                  1.000000                               1.0    1.000000  \n...                       ...                               ...         ...  \n4451                 0.333333                               0.5    0.250000  \n190                  1.000000                               1.0    5.500000  \n3967                 2.000000                               1.0    2.000000  \n3335                 2.000000                               2.0    1.250000  \n3646                 1.000000                               1.0    0.375000  \n\n[7716 rows x 214 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Height_cms</th>\n      <th>Weight_lbs</th>\n      <th>avg_BODY_att</th>\n      <th>avg_BODY_landed</th>\n      <th>avg_CLINCH_att</th>\n      <th>avg_CLINCH_landed</th>\n      <th>avg_DISTANCE_att</th>\n      <th>avg_DISTANCE_landed</th>\n      <th>avg_GROUND_att</th>\n      <th>avg_GROUND_landed</th>\n      <th>avg_HEAD_att</th>\n      <th>avg_HEAD_landed</th>\n      <th>avg_KD</th>\n      <th>avg_LEG_att</th>\n      <th>avg_LEG_landed</th>\n      <th>avg_PASS</th>\n      <th>avg_REV</th>\n      <th>avg_SIG_STR_att</th>\n      <th>avg_SIG_STR_landed</th>\n      <th>avg_SIG_STR_pct</th>\n      <th>avg_SIG_STatt</th>\n      <th>avg_SIG_STlanded</th>\n      <th>avg_SIG_STpct</th>\n      <th>avg_SUATT</th>\n      <th>avg_SUB_ATT</th>\n      <th>avg_TD_att</th>\n      <th>avg_TD_landed</th>\n      <th>avg_TD_pct</th>\n      <th>avg_TOTAL_STR_att</th>\n      <th>avg_TOTAL_STR_landed</th>\n      <th>avg_TOTAL_STatt</th>\n      <th>avg_TOTAL_STlanded</th>\n      <th>avg_opp_BODY_att</th>\n      <th>avg_opp_BODY_landed</th>\n      <th>avg_opp_CLINCH_att</th>\n      <th>avg_opp_CLINCH_landed</th>\n      <th>avg_opp_DISTANCE_att</th>\n      <th>avg_opp_DISTANCE_landed</th>\n      <th>avg_opp_GROUND_att</th>\n      <th>avg_opp_GROUND_landed</th>\n      <th>avg_opp_HEAD_att</th>\n      <th>avg_opp_HEAD_landed</th>\n      <th>avg_opp_KD</th>\n      <th>avg_opp_LEG_att</th>\n      <th>avg_opp_LEG_landed</th>\n      <th>avg_opp_PASS</th>\n      <th>avg_opp_REV</th>\n      <th>avg_opp_SIG_STR_att</th>\n      <th>avg_opp_SIG_STR_landed</th>\n      <th>avg_opp_SIG_STR_pct</th>\n      <th>avg_opp_SIG_STatt</th>\n      <th>avg_opp_SIG_STlanded</th>\n      <th>avg_opp_SIG_STpct</th>\n      <th>avg_opp_SUATT</th>\n      <th>avg_opp_SUB_ATT</th>\n      <th>avg_opp_TD_att</th>\n      <th>avg_opp_TD_landed</th>\n      <th>avg_opp_TD_pct</th>\n      <th>avg_opp_TOTAL_STR_att</th>\n      <th>avg_opp_TOTAL_STR_landed</th>\n      <th>avg_opp_TOTAL_STatt</th>\n      <th>avg_opp_TOTAL_STlanded</th>\n      <th>current_lose_streak</th>\n      <th>current_win_streak</th>\n      <th>draw</th>\n      <th>is_winner</th>\n      <th>location</th>\n      <th>longest_win_streak</th>\n      <th>losses</th>\n      <th>no_of_rounds</th>\n      <th>Height_cms_opponent</th>\n      <th>Stance_opponent</th>\n      <th>Weight_lbs_opponent</th>\n      <th>avg_BODY_att_opponent</th>\n      <th>avg_BODY_landed_opponent</th>\n      <th>avg_CLINCH_att_opponent</th>\n      <th>avg_CLINCH_landed_opponent</th>\n      <th>avg_DISTANCE_att_opponent</th>\n      <th>avg_DISTANCE_landed_opponent</th>\n      <th>avg_GROUND_att_opponent</th>\n      <th>avg_GROUND_landed_opponent</th>\n      <th>avg_HEAD_att_opponent</th>\n      <th>avg_HEAD_landed_opponent</th>\n      <th>avg_KD_opponent</th>\n      <th>avg_LEG_att_opponent</th>\n      <th>avg_LEG_landed_opponent</th>\n      <th>avg_PASS_opponent</th>\n      <th>avg_REV_opponent</th>\n      <th>avg_SIG_STR_att_opponent</th>\n      <th>avg_SIG_STR_landed_opponent</th>\n      <th>avg_SIG_STR_pct_opponent</th>\n      <th>avg_SUB_ATT_opponent</th>\n      <th>avg_TD_att_opponent</th>\n      <th>avg_TD_landed_opponent</th>\n      <th>avg_TD_pct_opponent</th>\n      <th>avg_TOTAL_STR_att_opponent</th>\n      <th>avg_TOTAL_STR_landed_opponent</th>\n      <th>avg_opp_BODY_att_opponent</th>\n      <th>avg_opp_BODY_landed_opponent</th>\n      <th>avg_opp_CLINCH_att_opponent</th>\n      <th>avg_opp_CLINCH_landed_opponent</th>\n      <th>avg_opp_DISTANCE_att_opponent</th>\n      <th>avg_opp_DISTANCE_landed_opponent</th>\n      <th>avg_opp_GROUND_att_opponent</th>\n      <th>avg_opp_GROUND_landed_opponent</th>\n      <th>avg_opp_HEAD_att_opponent</th>\n      <th>avg_opp_HEAD_landed_opponent</th>\n      <th>avg_opp_KD_opponent</th>\n      <th>avg_opp_LEG_att_opponent</th>\n      <th>avg_opp_LEG_landed_opponent</th>\n      <th>avg_opp_PASS_opponent</th>\n      <th>avg_opp_REV_opponent</th>\n      <th>avg_opp_SIG_STR_att_opponent</th>\n      <th>avg_opp_SIG_STR_landed_opponent</th>\n      <th>avg_opp_SIG_STR_pct_opponent</th>\n      <th>avg_opp_SUB_ATT_opponent</th>\n      <th>avg_opp_TD_att_opponent</th>\n      <th>avg_opp_TD_landed_opponent</th>\n      <th>avg_opp_TD_pct_opponent</th>\n      <th>avg_opp_TOTAL_STR_att_opponent</th>\n      <th>avg_opp_TOTAL_STR_landed_opponent</th>\n      <th>current_lose_streak_opponent</th>\n      <th>current_win_streak_opponent</th>\n      <th>draw_opponent</th>\n      <th>longest_win_streak_opponent</th>\n      <th>losses_opponent</th>\n      <th>total_rounds_fought_opponent</th>\n      <th>total_time_fought(seconds)_opponent</th>\n      <th>total_title_bouts_opponent</th>\n      <th>win_by_Decision_Majority_opponent</th>\n      <th>win_by_Decision_Split_opponent</th>\n      <th>win_by_Decision_Unanimous_opponent</th>\n      <th>win_by_KO/TKO_opponent</th>\n      <th>win_by_Submission_opponent</th>\n      <th>win_by_TKO_Doctor_Stoppage_opponent</th>\n      <th>wins_opponent</th>\n      <th>title_bout</th>\n      <th>total_rounds_fought</th>\n      <th>total_time_fought(seconds)</th>\n      <th>total_title_bouts</th>\n      <th>weight_class</th>\n      <th>win_by_Decision_Majority</th>\n      <th>win_by_Decision_Split</th>\n      <th>win_by_Decision_Unanimous</th>\n      <th>win_by_KO/TKO</th>\n      <th>win_by_Submission</th>\n      <th>win_by_TKO_Doctor_Stoppage</th>\n      <th>wins</th>\n      <th>Height_cms_ratio</th>\n      <th>Reach_cms_ratio</th>\n      <th>Weight_lbs_ratio</th>\n      <th>avg_BODY_att_ratio</th>\n      <th>avg_BODY_landed_ratio</th>\n      <th>avg_CLINCH_att_ratio</th>\n      <th>avg_CLINCH_landed_ratio</th>\n      <th>avg_DISTANCE_att_ratio</th>\n      <th>avg_DISTANCE_landed_ratio</th>\n      <th>avg_GROUND_att_ratio</th>\n      <th>avg_GROUND_landed_ratio</th>\n      <th>avg_HEAD_att_ratio</th>\n      <th>avg_HEAD_landed_ratio</th>\n      <th>avg_KD_ratio</th>\n      <th>avg_LEG_att_ratio</th>\n      <th>avg_LEG_landed_ratio</th>\n      <th>avg_PASS_ratio</th>\n      <th>avg_REV_ratio</th>\n      <th>avg_SIG_STR_att_ratio</th>\n      <th>avg_SIG_STR_landed_ratio</th>\n      <th>avg_SIG_STR_pct_ratio</th>\n      <th>avg_SUB_ATT_ratio</th>\n      <th>avg_TD_att_ratio</th>\n      <th>avg_TD_landed_ratio</th>\n      <th>avg_TD_pct_ratio</th>\n      <th>avg_TOTAL_STR_att_ratio</th>\n      <th>avg_TOTAL_STR_landed_ratio</th>\n      <th>avg_opp_BODY_att_ratio</th>\n      <th>avg_opp_BODY_landed_ratio</th>\n      <th>avg_opp_CLINCH_att_ratio</th>\n      <th>avg_opp_CLINCH_landed_ratio</th>\n      <th>avg_opp_DISTANCE_att_ratio</th>\n      <th>avg_opp_DISTANCE_landed_ratio</th>\n      <th>avg_opp_GROUND_att_ratio</th>\n      <th>avg_opp_GROUND_landed_ratio</th>\n      <th>avg_opp_HEAD_att_ratio</th>\n      <th>avg_opp_HEAD_landed_ratio</th>\n      <th>avg_opp_KD_ratio</th>\n      <th>avg_opp_LEG_att_ratio</th>\n      <th>avg_opp_LEG_landed_ratio</th>\n      <th>avg_opp_PASS_ratio</th>\n      <th>avg_opp_REV_ratio</th>\n      <th>avg_opp_SIG_STR_att_ratio</th>\n      <th>avg_opp_SIG_STR_landed_ratio</th>\n      <th>avg_opp_SIG_STR_pct_ratio</th>\n      <th>avg_opp_SUB_ATT_ratio</th>\n      <th>avg_opp_TD_att_ratio</th>\n      <th>avg_opp_TD_landed_ratio</th>\n      <th>avg_opp_TD_pct_ratio</th>\n      <th>avg_opp_TOTAL_STR_att_ratio</th>\n      <th>avg_opp_TOTAL_STR_landed_ratio</th>\n      <th>current_lose_streak_ratio</th>\n      <th>current_win_streak_ratio</th>\n      <th>draw_ratio</th>\n      <th>longest_win_streak_ratio</th>\n      <th>losses_ratio</th>\n      <th>total_rounds_fought_ratio</th>\n      <th>total_time_fought(seconds)_ratio</th>\n      <th>total_title_bouts_ratio</th>\n      <th>win_by_Decision_Majority_ratio</th>\n      <th>win_by_Decision_Split_ratio</th>\n      <th>win_by_Decision_Unanimous_ratio</th>\n      <th>win_by_KO/TKO_ratio</th>\n      <th>win_by_Submission_ratio</th>\n      <th>win_by_TKO_Doctor_Stoppage_ratio</th>\n      <th>wins_ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1201</td>\n      <td>177.80</td>\n      <td>155.0</td>\n      <td>8.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>24.000000</td>\n      <td>9.000000</td>\n      <td>17.000000</td>\n      <td>14.000000</td>\n      <td>25.000000</td>\n      <td>15.000000</td>\n      <td>0.000000</td>\n      <td>8.000000</td>\n      <td>7.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>41.000000</td>\n      <td>23.000000</td>\n      <td>0.560000</td>\n      <td>71.295658</td>\n      <td>31.374237</td>\n      <td>0.464392</td>\n      <td>1.000000</td>\n      <td>0.537707</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>0.330000</td>\n      <td>201.000000</td>\n      <td>169.000000</td>\n      <td>93.753403</td>\n      <td>51.441892</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>10.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>10.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>11.000000</td>\n      <td>2.000000</td>\n      <td>0.180000</td>\n      <td>65.95863</td>\n      <td>27.005665</td>\n      <td>0.414369</td>\n      <td>0.000000</td>\n      <td>0.453785</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>26.000000</td>\n      <td>13.00000</td>\n      <td>86.383180</td>\n      <td>45.278695</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>True</td>\n      <td>Las Vegas, Nevada, USA</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>175.26</td>\n      <td>Orthodox</td>\n      <td>155.0</td>\n      <td>7.352941</td>\n      <td>5.000000</td>\n      <td>6.764706</td>\n      <td>4.000000</td>\n      <td>67.823529</td>\n      <td>17.588235</td>\n      <td>9.058824</td>\n      <td>5.000000</td>\n      <td>74.882353</td>\n      <td>20.470588</td>\n      <td>0.352941</td>\n      <td>1.411765</td>\n      <td>1.117647</td>\n      <td>1.941176</td>\n      <td>0.235294</td>\n      <td>83.647059</td>\n      <td>26.588235</td>\n      <td>0.368235</td>\n      <td>0.588235</td>\n      <td>5.529412</td>\n      <td>2.294118</td>\n      <td>0.444118</td>\n      <td>100.941176</td>\n      <td>42.352941</td>\n      <td>10.176471</td>\n      <td>5.176471</td>\n      <td>8.882353</td>\n      <td>4.941176</td>\n      <td>94.882353</td>\n      <td>25.823529</td>\n      <td>2.470588</td>\n      <td>1.588235</td>\n      <td>90.294118</td>\n      <td>22.470588</td>\n      <td>0.352941</td>\n      <td>5.764706</td>\n      <td>4.705882</td>\n      <td>0.411765</td>\n      <td>0.117647</td>\n      <td>106.235294</td>\n      <td>32.352941</td>\n      <td>0.315294</td>\n      <td>0.294118</td>\n      <td>3.352941</td>\n      <td>0.588235</td>\n      <td>0.155882</td>\n      <td>124.235294</td>\n      <td>47.941176</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>8.0</td>\n      <td>7.0</td>\n      <td>48.0</td>\n      <td>780.117647</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>6.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>10.0</td>\n      <td>False</td>\n      <td>3.0</td>\n      <td>900.000000</td>\n      <td>1.0</td>\n      <td>Featherweight</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.014411</td>\n      <td>1.014206</td>\n      <td>1.000000</td>\n      <td>1.077465</td>\n      <td>0.333333</td>\n      <td>0.128788</td>\n      <td>0.200000</td>\n      <td>0.363248</td>\n      <td>0.537975</td>\n      <td>1.789474</td>\n      <td>2.500000</td>\n      <td>0.342636</td>\n      <td>0.745205</td>\n      <td>0.739130</td>\n      <td>3.731707</td>\n      <td>3.777778</td>\n      <td>0.680000</td>\n      <td>0.809524</td>\n      <td>0.496178</td>\n      <td>0.869936</td>\n      <td>1.140155</td>\n      <td>1.121309</td>\n      <td>0.612613</td>\n      <td>0.607143</td>\n      <td>0.920978</td>\n      <td>1.981535</td>\n      <td>3.921303</td>\n      <td>0.089474</td>\n      <td>0.161905</td>\n      <td>0.101190</td>\n      <td>0.168317</td>\n      <td>0.114724</td>\n      <td>0.074561</td>\n      <td>0.576271</td>\n      <td>0.772727</td>\n      <td>0.120490</td>\n      <td>0.085213</td>\n      <td>0.739130</td>\n      <td>0.295652</td>\n      <td>0.350515</td>\n      <td>0.708333</td>\n      <td>0.894737</td>\n      <td>0.111903</td>\n      <td>0.089947</td>\n      <td>0.897138</td>\n      <td>1.108284</td>\n      <td>0.229730</td>\n      <td>0.629630</td>\n      <td>0.865140</td>\n      <td>0.215594</td>\n      <td>0.286058</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.222222</td>\n      <td>0.125000</td>\n      <td>0.081633</td>\n      <td>1.153475</td>\n      <td>0.666667</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.285714</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.181818</td>\n    </tr>\n    <tr>\n      <td>3124</td>\n      <td>185.42</td>\n      <td>185.0</td>\n      <td>10.500000</td>\n      <td>8.416667</td>\n      <td>8.583333</td>\n      <td>6.083333</td>\n      <td>39.083333</td>\n      <td>20.833333</td>\n      <td>13.500000</td>\n      <td>7.916667</td>\n      <td>40.000000</td>\n      <td>18.416667</td>\n      <td>0.833333</td>\n      <td>10.666667</td>\n      <td>8.000000</td>\n      <td>3.166667</td>\n      <td>0.166667</td>\n      <td>69.893954</td>\n      <td>30.455255</td>\n      <td>0.455581</td>\n      <td>61.166667</td>\n      <td>34.833333</td>\n      <td>0.568333</td>\n      <td>0.550644</td>\n      <td>0.666667</td>\n      <td>1.583333</td>\n      <td>1.000000</td>\n      <td>0.387500</td>\n      <td>92.116042</td>\n      <td>50.280745</td>\n      <td>77.416667</td>\n      <td>50.500000</td>\n      <td>7.000000</td>\n      <td>4.250000</td>\n      <td>9.583333</td>\n      <td>6.666667</td>\n      <td>33.750000</td>\n      <td>9.500000</td>\n      <td>1.166667</td>\n      <td>0.833333</td>\n      <td>26.833333</td>\n      <td>5.750000</td>\n      <td>0.166667</td>\n      <td>10.666667</td>\n      <td>7.000000</td>\n      <td>0.250000</td>\n      <td>0.000000</td>\n      <td>66.562136</td>\n      <td>27.726414</td>\n      <td>0.426225</td>\n      <td>44.50000</td>\n      <td>17.000000</td>\n      <td>0.325833</td>\n      <td>0.470368</td>\n      <td>0.416667</td>\n      <td>3.500000</td>\n      <td>0.583333</td>\n      <td>0.195000</td>\n      <td>86.897345</td>\n      <td>45.92459</td>\n      <td>59.583333</td>\n      <td>31.333333</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>True</td>\n      <td>Los Angeles, California, USA</td>\n      <td>8.0</td>\n      <td>3.0</td>\n      <td>3</td>\n      <td>187.96</td>\n      <td>Orthodox</td>\n      <td>205.0</td>\n      <td>3.666667</td>\n      <td>2.888889</td>\n      <td>5.666667</td>\n      <td>3.666667</td>\n      <td>48.333333</td>\n      <td>14.111111</td>\n      <td>12.777778</td>\n      <td>8.000000</td>\n      <td>56.555556</td>\n      <td>17.333333</td>\n      <td>0.555556</td>\n      <td>6.555556</td>\n      <td>5.555556</td>\n      <td>1.888889</td>\n      <td>0.000000</td>\n      <td>66.777778</td>\n      <td>25.777778</td>\n      <td>0.374444</td>\n      <td>0.222222</td>\n      <td>5.222222</td>\n      <td>2.222222</td>\n      <td>0.282222</td>\n      <td>90.444444</td>\n      <td>47.111111</td>\n      <td>4.777778</td>\n      <td>3.000000</td>\n      <td>2.222222</td>\n      <td>1.444444</td>\n      <td>50.111111</td>\n      <td>10.888889</td>\n      <td>2.111111</td>\n      <td>1.444444</td>\n      <td>46.444444</td>\n      <td>8.555556</td>\n      <td>0.111111</td>\n      <td>3.222222</td>\n      <td>2.222222</td>\n      <td>0.222222</td>\n      <td>0.000000</td>\n      <td>54.444444</td>\n      <td>13.777778</td>\n      <td>0.261111</td>\n      <td>0.555556</td>\n      <td>0.666667</td>\n      <td>0.333333</td>\n      <td>0.138889</td>\n      <td>62.555556</td>\n      <td>21.666667</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>20.0</td>\n      <td>580.111111</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>False</td>\n      <td>30.0</td>\n      <td>711.833333</td>\n      <td>4.0</td>\n      <td>Light Heavyweight</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>9.0</td>\n      <td>0.986558</td>\n      <td>1.000000</td>\n      <td>0.902913</td>\n      <td>2.464286</td>\n      <td>2.421429</td>\n      <td>1.437500</td>\n      <td>1.517857</td>\n      <td>0.812500</td>\n      <td>1.444853</td>\n      <td>1.052419</td>\n      <td>0.990741</td>\n      <td>0.712355</td>\n      <td>1.059091</td>\n      <td>1.178571</td>\n      <td>1.544118</td>\n      <td>1.372881</td>\n      <td>1.442308</td>\n      <td>1.166667</td>\n      <td>1.430333</td>\n      <td>1.392688</td>\n      <td>1.001652</td>\n      <td>1.363636</td>\n      <td>0.415179</td>\n      <td>0.620690</td>\n      <td>1.082106</td>\n      <td>1.406711</td>\n      <td>1.452297</td>\n      <td>1.384615</td>\n      <td>1.312500</td>\n      <td>3.284483</td>\n      <td>3.136364</td>\n      <td>0.679891</td>\n      <td>0.883178</td>\n      <td>0.696429</td>\n      <td>0.750000</td>\n      <td>0.586651</td>\n      <td>0.706395</td>\n      <td>1.050000</td>\n      <td>2.763158</td>\n      <td>2.482759</td>\n      <td>1.022727</td>\n      <td>1.000000</td>\n      <td>1.561574</td>\n      <td>1.510316</td>\n      <td>1.015809</td>\n      <td>0.910714</td>\n      <td>2.700000</td>\n      <td>1.187500</td>\n      <td>1.049268</td>\n      <td>1.562048</td>\n      <td>1.619928</td>\n      <td>2.0</td>\n      <td>0.333333</td>\n      <td>1.0</td>\n      <td>1.500000</td>\n      <td>1.333333</td>\n      <td>1.476190</td>\n      <td>1.226673</td>\n      <td>2.500000</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.200000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>1.0</td>\n      <td>1.250000</td>\n    </tr>\n    <tr>\n      <td>1709</td>\n      <td>167.64</td>\n      <td>170.0</td>\n      <td>8.688929</td>\n      <td>6.057405</td>\n      <td>8.088633</td>\n      <td>5.444747</td>\n      <td>53.363434</td>\n      <td>19.423322</td>\n      <td>9.195929</td>\n      <td>6.081550</td>\n      <td>55.841688</td>\n      <td>20.061186</td>\n      <td>0.255765</td>\n      <td>6.117379</td>\n      <td>4.831027</td>\n      <td>1.348541</td>\n      <td>0.160135</td>\n      <td>69.893954</td>\n      <td>30.455255</td>\n      <td>0.455581</td>\n      <td>71.295658</td>\n      <td>31.374237</td>\n      <td>0.464392</td>\n      <td>0.550644</td>\n      <td>0.537707</td>\n      <td>2.898266</td>\n      <td>1.246699</td>\n      <td>0.323823</td>\n      <td>92.116042</td>\n      <td>50.280745</td>\n      <td>93.753403</td>\n      <td>51.441892</td>\n      <td>8.234617</td>\n      <td>5.558208</td>\n      <td>7.367146</td>\n      <td>4.843631</td>\n      <td>52.060986</td>\n      <td>18.097636</td>\n      <td>6.809350</td>\n      <td>4.397422</td>\n      <td>52.022985</td>\n      <td>17.110886</td>\n      <td>0.155104</td>\n      <td>5.979880</td>\n      <td>4.669595</td>\n      <td>1.047956</td>\n      <td>0.157535</td>\n      <td>66.562136</td>\n      <td>27.726414</td>\n      <td>0.426225</td>\n      <td>65.95863</td>\n      <td>27.005665</td>\n      <td>0.414369</td>\n      <td>0.470368</td>\n      <td>0.453785</td>\n      <td>2.864079</td>\n      <td>1.057395</td>\n      <td>0.266356</td>\n      <td>86.897345</td>\n      <td>45.92459</td>\n      <td>86.383180</td>\n      <td>45.278695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>False</td>\n      <td>Monterrey, Nuevo Leon, Mexico</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>182.88</td>\n      <td>NaN</td>\n      <td>155.0</td>\n      <td>8.724245</td>\n      <td>6.088775</td>\n      <td>8.183586</td>\n      <td>5.517398</td>\n      <td>53.642673</td>\n      <td>19.536763</td>\n      <td>9.160172</td>\n      <td>6.059382</td>\n      <td>56.072199</td>\n      <td>20.133278</td>\n      <td>0.255308</td>\n      <td>6.189988</td>\n      <td>4.891489</td>\n      <td>1.347048</td>\n      <td>0.163136</td>\n      <td>70.986432</td>\n      <td>31.113543</td>\n      <td>0.459137</td>\n      <td>0.547939</td>\n      <td>2.930322</td>\n      <td>1.253133</td>\n      <td>0.326234</td>\n      <td>93.538810</td>\n      <td>51.220837</td>\n      <td>8.257727</td>\n      <td>5.584944</td>\n      <td>7.424789</td>\n      <td>4.878006</td>\n      <td>52.517953</td>\n      <td>18.215220</td>\n      <td>6.868664</td>\n      <td>4.464795</td>\n      <td>52.533934</td>\n      <td>17.265744</td>\n      <td>0.156717</td>\n      <td>6.019744</td>\n      <td>4.707333</td>\n      <td>1.052897</td>\n      <td>0.154548</td>\n      <td>66.811405</td>\n      <td>27.558020</td>\n      <td>0.419474</td>\n      <td>0.452124</td>\n      <td>2.849548</td>\n      <td>1.054162</td>\n      <td>0.265772</td>\n      <td>87.214914</td>\n      <td>45.810098</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>598.392405</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>596.979623</td>\n      <td>0.0</td>\n      <td>Welterweight</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.917120</td>\n      <td>0.973116</td>\n      <td>1.096154</td>\n      <td>1.607005</td>\n      <td>1.555439</td>\n      <td>1.853073</td>\n      <td>1.721062</td>\n      <td>1.879329</td>\n      <td>1.756918</td>\n      <td>2.110335</td>\n      <td>1.850151</td>\n      <td>1.673061</td>\n      <td>1.556839</td>\n      <td>1.050136</td>\n      <td>1.778599</td>\n      <td>1.679467</td>\n      <td>1.277408</td>\n      <td>1.037172</td>\n      <td>1.430333</td>\n      <td>1.392688</td>\n      <td>1.001652</td>\n      <td>1.121309</td>\n      <td>1.464598</td>\n      <td>1.233509</td>\n      <td>1.028383</td>\n      <td>1.406711</td>\n      <td>1.452297</td>\n      <td>1.514798</td>\n      <td>1.466022</td>\n      <td>1.798243</td>\n      <td>1.673771</td>\n      <td>1.829816</td>\n      <td>1.717552</td>\n      <td>2.237167</td>\n      <td>1.895853</td>\n      <td>1.671125</td>\n      <td>1.603145</td>\n      <td>1.029811</td>\n      <td>1.588474</td>\n      <td>1.520515</td>\n      <td>1.244199</td>\n      <td>1.038117</td>\n      <td>1.561574</td>\n      <td>1.510316</td>\n      <td>1.015809</td>\n      <td>1.108284</td>\n      <td>1.408409</td>\n      <td>1.200573</td>\n      <td>1.027018</td>\n      <td>1.562048</td>\n      <td>1.619928</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.392094</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>3974</td>\n      <td>187.96</td>\n      <td>205.0</td>\n      <td>6.800000</td>\n      <td>5.000000</td>\n      <td>8.400000</td>\n      <td>5.800000</td>\n      <td>17.400000</td>\n      <td>6.600000</td>\n      <td>9.000000</td>\n      <td>6.000000</td>\n      <td>25.800000</td>\n      <td>12.400000</td>\n      <td>0.200000</td>\n      <td>2.200000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>69.893954</td>\n      <td>30.455255</td>\n      <td>0.455581</td>\n      <td>34.800000</td>\n      <td>18.400000</td>\n      <td>0.618000</td>\n      <td>0.550644</td>\n      <td>0.400000</td>\n      <td>1.800000</td>\n      <td>0.400000</td>\n      <td>0.300000</td>\n      <td>92.116042</td>\n      <td>50.280745</td>\n      <td>41.800000</td>\n      <td>25.200000</td>\n      <td>4.200000</td>\n      <td>3.200000</td>\n      <td>7.800000</td>\n      <td>4.400000</td>\n      <td>25.600000</td>\n      <td>7.800000</td>\n      <td>4.800000</td>\n      <td>3.200000</td>\n      <td>31.000000</td>\n      <td>9.600000</td>\n      <td>0.600000</td>\n      <td>3.000000</td>\n      <td>2.600000</td>\n      <td>0.200000</td>\n      <td>0.000000</td>\n      <td>66.562136</td>\n      <td>27.726414</td>\n      <td>0.426225</td>\n      <td>38.20000</td>\n      <td>15.400000</td>\n      <td>0.468000</td>\n      <td>0.470368</td>\n      <td>0.200000</td>\n      <td>1.600000</td>\n      <td>1.000000</td>\n      <td>0.532000</td>\n      <td>86.897345</td>\n      <td>45.92459</td>\n      <td>42.800000</td>\n      <td>19.800000</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>True</td>\n      <td>Portland, Oregon, USA</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>187.96</td>\n      <td>Orthodox</td>\n      <td>185.0</td>\n      <td>7.200000</td>\n      <td>6.200000</td>\n      <td>9.800000</td>\n      <td>5.600000</td>\n      <td>98.900000</td>\n      <td>37.600000</td>\n      <td>4.400000</td>\n      <td>2.300000</td>\n      <td>91.200000</td>\n      <td>26.600000</td>\n      <td>0.300000</td>\n      <td>14.700000</td>\n      <td>12.700000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>113.100000</td>\n      <td>45.500000</td>\n      <td>0.418000</td>\n      <td>0.000000</td>\n      <td>0.900000</td>\n      <td>0.300000</td>\n      <td>0.150000</td>\n      <td>122.100000</td>\n      <td>54.200000</td>\n      <td>5.100000</td>\n      <td>4.300000</td>\n      <td>6.300000</td>\n      <td>4.000000</td>\n      <td>71.800000</td>\n      <td>23.000000</td>\n      <td>3.700000</td>\n      <td>2.600000</td>\n      <td>69.300000</td>\n      <td>19.200000</td>\n      <td>0.800000</td>\n      <td>7.400000</td>\n      <td>6.100000</td>\n      <td>0.600000</td>\n      <td>0.000000</td>\n      <td>81.800000</td>\n      <td>29.600000</td>\n      <td>0.413000</td>\n      <td>0.100000</td>\n      <td>2.500000</td>\n      <td>1.000000</td>\n      <td>0.273000</td>\n      <td>88.500000</td>\n      <td>36.100000</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>23.0</td>\n      <td>627.300000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6.0</td>\n      <td>False</td>\n      <td>6.0</td>\n      <td>255.400000</td>\n      <td>0.0</td>\n      <td>Light Heavyweight</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>1.000000</td>\n      <td>0.973820</td>\n      <td>1.107527</td>\n      <td>0.951220</td>\n      <td>0.833333</td>\n      <td>0.870370</td>\n      <td>1.030303</td>\n      <td>0.184184</td>\n      <td>0.196891</td>\n      <td>1.851852</td>\n      <td>2.121212</td>\n      <td>0.290672</td>\n      <td>0.485507</td>\n      <td>0.923077</td>\n      <td>0.203822</td>\n      <td>0.145985</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>1.430333</td>\n      <td>1.392688</td>\n      <td>1.001652</td>\n      <td>1.400000</td>\n      <td>1.473684</td>\n      <td>1.076923</td>\n      <td>1.130435</td>\n      <td>1.406711</td>\n      <td>1.452297</td>\n      <td>0.852459</td>\n      <td>0.792453</td>\n      <td>1.205479</td>\n      <td>1.080000</td>\n      <td>0.365385</td>\n      <td>0.366667</td>\n      <td>1.234043</td>\n      <td>1.166667</td>\n      <td>0.455192</td>\n      <td>0.524752</td>\n      <td>0.888889</td>\n      <td>0.476190</td>\n      <td>0.507042</td>\n      <td>0.750000</td>\n      <td>1.000000</td>\n      <td>1.561574</td>\n      <td>1.510316</td>\n      <td>1.015809</td>\n      <td>1.090909</td>\n      <td>0.742857</td>\n      <td>1.000000</td>\n      <td>1.203456</td>\n      <td>1.562048</td>\n      <td>1.619928</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>1.666667</td>\n      <td>0.400000</td>\n      <td>0.291667</td>\n      <td>0.408085</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>1.666667</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.714286</td>\n    </tr>\n    <tr>\n      <td>274</td>\n      <td>182.88</td>\n      <td>265.0</td>\n      <td>8.688929</td>\n      <td>6.057405</td>\n      <td>8.088633</td>\n      <td>5.444747</td>\n      <td>53.363434</td>\n      <td>19.423322</td>\n      <td>9.195929</td>\n      <td>6.081550</td>\n      <td>55.841688</td>\n      <td>20.061186</td>\n      <td>0.255765</td>\n      <td>6.117379</td>\n      <td>4.831027</td>\n      <td>1.348541</td>\n      <td>0.160135</td>\n      <td>69.893954</td>\n      <td>30.455255</td>\n      <td>0.455581</td>\n      <td>71.295658</td>\n      <td>31.374237</td>\n      <td>0.464392</td>\n      <td>0.550644</td>\n      <td>0.537707</td>\n      <td>2.898266</td>\n      <td>1.246699</td>\n      <td>0.323823</td>\n      <td>92.116042</td>\n      <td>50.280745</td>\n      <td>93.753403</td>\n      <td>51.441892</td>\n      <td>8.234617</td>\n      <td>5.558208</td>\n      <td>7.367146</td>\n      <td>4.843631</td>\n      <td>52.060986</td>\n      <td>18.097636</td>\n      <td>6.809350</td>\n      <td>4.397422</td>\n      <td>52.022985</td>\n      <td>17.110886</td>\n      <td>0.155104</td>\n      <td>5.979880</td>\n      <td>4.669595</td>\n      <td>1.047956</td>\n      <td>0.157535</td>\n      <td>66.562136</td>\n      <td>27.726414</td>\n      <td>0.426225</td>\n      <td>65.95863</td>\n      <td>27.005665</td>\n      <td>0.414369</td>\n      <td>0.470368</td>\n      <td>0.453785</td>\n      <td>2.864079</td>\n      <td>1.057395</td>\n      <td>0.266356</td>\n      <td>86.897345</td>\n      <td>45.92459</td>\n      <td>86.383180</td>\n      <td>45.278695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>False</td>\n      <td>Las Vegas, Nevada, USA</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>193.04</td>\n      <td>NaN</td>\n      <td>250.0</td>\n      <td>8.724245</td>\n      <td>6.088775</td>\n      <td>8.183586</td>\n      <td>5.517398</td>\n      <td>53.642673</td>\n      <td>19.536763</td>\n      <td>9.160172</td>\n      <td>6.059382</td>\n      <td>56.072199</td>\n      <td>20.133278</td>\n      <td>0.255308</td>\n      <td>6.189988</td>\n      <td>4.891489</td>\n      <td>1.347048</td>\n      <td>0.163136</td>\n      <td>70.986432</td>\n      <td>31.113543</td>\n      <td>0.459137</td>\n      <td>0.547939</td>\n      <td>2.930322</td>\n      <td>1.253133</td>\n      <td>0.326234</td>\n      <td>93.538810</td>\n      <td>51.220837</td>\n      <td>8.257727</td>\n      <td>5.584944</td>\n      <td>7.424789</td>\n      <td>4.878006</td>\n      <td>52.517953</td>\n      <td>18.215220</td>\n      <td>6.868664</td>\n      <td>4.464795</td>\n      <td>52.533934</td>\n      <td>17.265744</td>\n      <td>0.156717</td>\n      <td>6.019744</td>\n      <td>4.707333</td>\n      <td>1.052897</td>\n      <td>0.154548</td>\n      <td>66.811405</td>\n      <td>27.558020</td>\n      <td>0.419474</td>\n      <td>0.452124</td>\n      <td>2.849548</td>\n      <td>1.054162</td>\n      <td>0.265772</td>\n      <td>87.214914</td>\n      <td>45.810098</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>598.392405</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>True</td>\n      <td>0.0</td>\n      <td>596.979623</td>\n      <td>0.0</td>\n      <td>Heavyweight</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.947640</td>\n      <td>0.915042</td>\n      <td>1.059761</td>\n      <td>1.607005</td>\n      <td>1.555439</td>\n      <td>1.853073</td>\n      <td>1.721062</td>\n      <td>1.879329</td>\n      <td>1.756918</td>\n      <td>2.110335</td>\n      <td>1.850151</td>\n      <td>1.673061</td>\n      <td>1.556839</td>\n      <td>1.050136</td>\n      <td>1.778599</td>\n      <td>1.679467</td>\n      <td>1.277408</td>\n      <td>1.037172</td>\n      <td>1.430333</td>\n      <td>1.392688</td>\n      <td>1.001652</td>\n      <td>1.121309</td>\n      <td>1.464598</td>\n      <td>1.233509</td>\n      <td>1.028383</td>\n      <td>1.406711</td>\n      <td>1.452297</td>\n      <td>1.514798</td>\n      <td>1.466022</td>\n      <td>1.798243</td>\n      <td>1.673771</td>\n      <td>1.829816</td>\n      <td>1.717552</td>\n      <td>2.237167</td>\n      <td>1.895853</td>\n      <td>1.671125</td>\n      <td>1.603145</td>\n      <td>1.029811</td>\n      <td>1.588474</td>\n      <td>1.520515</td>\n      <td>1.244199</td>\n      <td>1.038117</td>\n      <td>1.561574</td>\n      <td>1.510316</td>\n      <td>1.015809</td>\n      <td>1.108284</td>\n      <td>1.408409</td>\n      <td>1.200573</td>\n      <td>1.027018</td>\n      <td>1.562048</td>\n      <td>1.619928</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.392094</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>4451</td>\n      <td>170.18</td>\n      <td>155.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>4.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>1.000000</td>\n      <td>0.200000</td>\n      <td>71.295658</td>\n      <td>31.374237</td>\n      <td>0.464392</td>\n      <td>0.000000</td>\n      <td>0.537707</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>13.000000</td>\n      <td>6.000000</td>\n      <td>93.753403</td>\n      <td>51.441892</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>65.95863</td>\n      <td>27.005665</td>\n      <td>0.414369</td>\n      <td>3.000000</td>\n      <td>0.453785</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>2.00000</td>\n      <td>86.383180</td>\n      <td>45.278695</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>False</td>\n      <td>Las Vegas, Nevada, USA</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>177.80</td>\n      <td>Southpaw</td>\n      <td>145.0</td>\n      <td>8.200000</td>\n      <td>6.600000</td>\n      <td>7.200000</td>\n      <td>4.800000</td>\n      <td>14.000000</td>\n      <td>6.200000</td>\n      <td>6.800000</td>\n      <td>4.600000</td>\n      <td>18.800000</td>\n      <td>8.400000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.600000</td>\n      <td>1.600000</td>\n      <td>0.200000</td>\n      <td>28.000000</td>\n      <td>15.600000</td>\n      <td>0.548000</td>\n      <td>1.400000</td>\n      <td>2.600000</td>\n      <td>1.000000</td>\n      <td>0.274000</td>\n      <td>52.200000</td>\n      <td>38.600000</td>\n      <td>1.600000</td>\n      <td>1.200000</td>\n      <td>9.200000</td>\n      <td>4.400000</td>\n      <td>11.800000</td>\n      <td>2.800000</td>\n      <td>21.000000</td>\n      <td>12.800000</td>\n      <td>39.200000</td>\n      <td>17.800000</td>\n      <td>0.000000</td>\n      <td>1.200000</td>\n      <td>1.000000</td>\n      <td>3.200000</td>\n      <td>0.000000</td>\n      <td>42.000000</td>\n      <td>20.000000</td>\n      <td>0.406000</td>\n      <td>0.200000</td>\n      <td>3.000000</td>\n      <td>1.800000</td>\n      <td>0.222000</td>\n      <td>67.200000</td>\n      <td>42.600000</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>11.0</td>\n      <td>516.800000</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>127.000000</td>\n      <td>0.0</td>\n      <td>Lightweight</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.957383</td>\n      <td>1.000950</td>\n      <td>1.068493</td>\n      <td>0.217391</td>\n      <td>0.263158</td>\n      <td>0.121951</td>\n      <td>0.172414</td>\n      <td>0.200000</td>\n      <td>0.138889</td>\n      <td>0.512821</td>\n      <td>0.357143</td>\n      <td>0.252525</td>\n      <td>0.106383</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.625000</td>\n      <td>0.769231</td>\n      <td>0.833333</td>\n      <td>0.206897</td>\n      <td>0.120482</td>\n      <td>0.775194</td>\n      <td>1.121309</td>\n      <td>0.555556</td>\n      <td>1.000000</td>\n      <td>1.569859</td>\n      <td>0.263158</td>\n      <td>0.176768</td>\n      <td>0.769231</td>\n      <td>0.909091</td>\n      <td>0.196078</td>\n      <td>0.370370</td>\n      <td>0.078125</td>\n      <td>0.263158</td>\n      <td>0.045455</td>\n      <td>0.072464</td>\n      <td>0.024876</td>\n      <td>0.053191</td>\n      <td>1.000000</td>\n      <td>0.454545</td>\n      <td>0.500000</td>\n      <td>0.238095</td>\n      <td>1.000000</td>\n      <td>0.046512</td>\n      <td>0.095238</td>\n      <td>1.422475</td>\n      <td>1.108284</td>\n      <td>0.250000</td>\n      <td>0.357143</td>\n      <td>0.818331</td>\n      <td>0.043988</td>\n      <td>0.068807</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.250000</td>\n      <td>0.666667</td>\n      <td>0.166667</td>\n      <td>0.247200</td>\n      <td>0.333333</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.333333</td>\n      <td>0.5</td>\n      <td>0.250000</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>187.96</td>\n      <td>205.0</td>\n      <td>6.687500</td>\n      <td>5.500000</td>\n      <td>10.937500</td>\n      <td>7.750000</td>\n      <td>53.437500</td>\n      <td>21.062500</td>\n      <td>2.500000</td>\n      <td>1.625000</td>\n      <td>53.937500</td>\n      <td>20.062500</td>\n      <td>0.437500</td>\n      <td>6.250000</td>\n      <td>4.875000</td>\n      <td>0.187500</td>\n      <td>0.000000</td>\n      <td>66.875000</td>\n      <td>30.437500</td>\n      <td>0.486250</td>\n      <td>71.295658</td>\n      <td>31.374237</td>\n      <td>0.464392</td>\n      <td>0.062500</td>\n      <td>0.537707</td>\n      <td>0.250000</td>\n      <td>0.062500</td>\n      <td>0.062500</td>\n      <td>74.875000</td>\n      <td>37.812500</td>\n      <td>93.753403</td>\n      <td>51.441892</td>\n      <td>12.437500</td>\n      <td>6.187500</td>\n      <td>6.750000</td>\n      <td>4.500000</td>\n      <td>61.937500</td>\n      <td>26.187500</td>\n      <td>0.312500</td>\n      <td>0.187500</td>\n      <td>45.000000</td>\n      <td>15.062500</td>\n      <td>0.062500</td>\n      <td>11.562500</td>\n      <td>9.625000</td>\n      <td>0.187500</td>\n      <td>0.000000</td>\n      <td>69.000000</td>\n      <td>30.875000</td>\n      <td>0.476250</td>\n      <td>65.95863</td>\n      <td>27.005665</td>\n      <td>0.414369</td>\n      <td>0.000000</td>\n      <td>0.453785</td>\n      <td>2.750000</td>\n      <td>0.437500</td>\n      <td>0.103750</td>\n      <td>80.000000</td>\n      <td>41.31250</td>\n      <td>86.383180</td>\n      <td>45.278695</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>False</td>\n      <td>Melbourne, Victoria, Australia</td>\n      <td>4.0</td>\n      <td>6.0</td>\n      <td>3</td>\n      <td>187.96</td>\n      <td>Orthodox</td>\n      <td>205.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>14.000000</td>\n      <td>2.000000</td>\n      <td>24.000000</td>\n      <td>15.000000</td>\n      <td>36.000000</td>\n      <td>16.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>9.000000</td>\n      <td>2.000000</td>\n      <td>38.000000</td>\n      <td>17.000000</td>\n      <td>0.440000</td>\n      <td>4.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>75.000000</td>\n      <td>49.000000</td>\n      <td>6.000000</td>\n      <td>5.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>18.000000</td>\n      <td>8.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>16.000000</td>\n      <td>7.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>22.000000</td>\n      <td>12.000000</td>\n      <td>0.540000</td>\n      <td>0.000000</td>\n      <td>7.000000</td>\n      <td>3.000000</td>\n      <td>0.420000</td>\n      <td>57.000000</td>\n      <td>45.000000</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>891.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>False</td>\n      <td>34.0</td>\n      <td>570.000000</td>\n      <td>0.0</td>\n      <td>Light Heavyweight</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>10.0</td>\n      <td>1.000000</td>\n      <td>1.013442</td>\n      <td>1.000000</td>\n      <td>7.687500</td>\n      <td>6.500000</td>\n      <td>11.937500</td>\n      <td>8.750000</td>\n      <td>3.629167</td>\n      <td>7.354167</td>\n      <td>0.140000</td>\n      <td>0.164062</td>\n      <td>1.484797</td>\n      <td>1.238971</td>\n      <td>1.437500</td>\n      <td>2.416667</td>\n      <td>2.937500</td>\n      <td>0.118750</td>\n      <td>0.333333</td>\n      <td>1.740385</td>\n      <td>1.746528</td>\n      <td>1.032118</td>\n      <td>1.121309</td>\n      <td>0.625000</td>\n      <td>0.531250</td>\n      <td>0.531250</td>\n      <td>0.998355</td>\n      <td>0.776250</td>\n      <td>1.919643</td>\n      <td>1.197917</td>\n      <td>3.875000</td>\n      <td>2.750000</td>\n      <td>3.312500</td>\n      <td>3.020833</td>\n      <td>0.328125</td>\n      <td>0.296875</td>\n      <td>2.705882</td>\n      <td>2.007812</td>\n      <td>1.062500</td>\n      <td>12.562500</td>\n      <td>10.625000</td>\n      <td>0.296875</td>\n      <td>0.500000</td>\n      <td>3.043478</td>\n      <td>2.451923</td>\n      <td>0.958604</td>\n      <td>1.108284</td>\n      <td>0.468750</td>\n      <td>0.359375</td>\n      <td>0.777289</td>\n      <td>1.396552</td>\n      <td>0.919837</td>\n      <td>2.0</td>\n      <td>0.500000</td>\n      <td>1.0</td>\n      <td>2.500000</td>\n      <td>7.000000</td>\n      <td>8.750000</td>\n      <td>0.640135</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>6.000000</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>5.500000</td>\n    </tr>\n    <tr>\n      <td>3967</td>\n      <td>187.96</td>\n      <td>185.0</td>\n      <td>11.500000</td>\n      <td>10.000000</td>\n      <td>1.500000</td>\n      <td>1.000000</td>\n      <td>106.000000</td>\n      <td>40.000000</td>\n      <td>1.500000</td>\n      <td>1.000000</td>\n      <td>86.500000</td>\n      <td>23.000000</td>\n      <td>0.000000</td>\n      <td>11.000000</td>\n      <td>9.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>109.000000</td>\n      <td>42.000000</td>\n      <td>0.435000</td>\n      <td>71.295658</td>\n      <td>31.374237</td>\n      <td>0.464392</td>\n      <td>0.500000</td>\n      <td>0.537707</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>111.000000</td>\n      <td>44.000000</td>\n      <td>93.753403</td>\n      <td>51.441892</td>\n      <td>13.000000</td>\n      <td>10.500000</td>\n      <td>5.000000</td>\n      <td>3.000000</td>\n      <td>134.500000</td>\n      <td>47.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>122.000000</td>\n      <td>35.500000</td>\n      <td>0.000000</td>\n      <td>4.500000</td>\n      <td>4.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>139.500000</td>\n      <td>50.000000</td>\n      <td>0.310000</td>\n      <td>65.95863</td>\n      <td>27.005665</td>\n      <td>0.414369</td>\n      <td>0.000000</td>\n      <td>0.453785</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>140.500000</td>\n      <td>51.00000</td>\n      <td>86.383180</td>\n      <td>45.278695</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>False</td>\n      <td>Oklahoma City, Oklahoma, USA</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>185.42</td>\n      <td>Orthodox</td>\n      <td>205.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>5.000000</td>\n      <td>4.000000</td>\n      <td>13.000000</td>\n      <td>4.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>13.000000</td>\n      <td>4.000000</td>\n      <td>0.000000</td>\n      <td>4.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>18.000000</td>\n      <td>8.000000</td>\n      <td>0.440000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>24.000000</td>\n      <td>14.000000</td>\n      <td>4.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>9.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>4.000000</td>\n      <td>0.000000</td>\n      <td>11.000000</td>\n      <td>3.000000</td>\n      <td>0.270000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>18.000000</td>\n      <td>10.000000</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>233.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>False</td>\n      <td>4.0</td>\n      <td>572.000000</td>\n      <td>0.0</td>\n      <td>Light Heavyweight</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.013625</td>\n      <td>1.013442</td>\n      <td>0.902913</td>\n      <td>6.250000</td>\n      <td>5.500000</td>\n      <td>0.416667</td>\n      <td>0.400000</td>\n      <td>7.642857</td>\n      <td>8.200000</td>\n      <td>2.500000</td>\n      <td>2.000000</td>\n      <td>6.250000</td>\n      <td>4.800000</td>\n      <td>1.000000</td>\n      <td>2.400000</td>\n      <td>2.500000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>5.789474</td>\n      <td>4.777778</td>\n      <td>0.996528</td>\n      <td>1.121309</td>\n      <td>1.500000</td>\n      <td>1.500000</td>\n      <td>1.500000</td>\n      <td>4.480000</td>\n      <td>3.000000</td>\n      <td>2.800000</td>\n      <td>3.833333</td>\n      <td>2.000000</td>\n      <td>4.000000</td>\n      <td>13.550000</td>\n      <td>12.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>20.500000</td>\n      <td>36.500000</td>\n      <td>1.000000</td>\n      <td>1.833333</td>\n      <td>2.500000</td>\n      <td>0.200000</td>\n      <td>1.000000</td>\n      <td>11.708333</td>\n      <td>12.750000</td>\n      <td>1.031496</td>\n      <td>1.108284</td>\n      <td>0.666667</td>\n      <td>0.333333</td>\n      <td>0.500000</td>\n      <td>7.447368</td>\n      <td>4.727273</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>2.500000</td>\n      <td>2.448718</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>1.0</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <td>3335</td>\n      <td>182.88</td>\n      <td>185.0</td>\n      <td>0.571429</td>\n      <td>0.428571</td>\n      <td>4.571429</td>\n      <td>2.571429</td>\n      <td>8.428571</td>\n      <td>3.071429</td>\n      <td>5.571429</td>\n      <td>3.857143</td>\n      <td>16.714286</td>\n      <td>8.000000</td>\n      <td>0.428571</td>\n      <td>1.285714</td>\n      <td>1.071429</td>\n      <td>0.642857</td>\n      <td>0.071429</td>\n      <td>69.893954</td>\n      <td>30.455255</td>\n      <td>0.455581</td>\n      <td>18.571429</td>\n      <td>9.500000</td>\n      <td>0.518571</td>\n      <td>0.550644</td>\n      <td>0.428571</td>\n      <td>0.571429</td>\n      <td>0.428571</td>\n      <td>0.196429</td>\n      <td>92.116042</td>\n      <td>50.280745</td>\n      <td>22.500000</td>\n      <td>13.214286</td>\n      <td>1.785714</td>\n      <td>1.285714</td>\n      <td>6.357143</td>\n      <td>4.785714</td>\n      <td>8.428571</td>\n      <td>2.571429</td>\n      <td>6.714286</td>\n      <td>4.928571</td>\n      <td>15.285714</td>\n      <td>7.285714</td>\n      <td>0.142857</td>\n      <td>4.428571</td>\n      <td>3.714286</td>\n      <td>0.428571</td>\n      <td>0.071429</td>\n      <td>66.562136</td>\n      <td>27.726414</td>\n      <td>0.426225</td>\n      <td>21.50000</td>\n      <td>12.285714</td>\n      <td>0.447857</td>\n      <td>0.470368</td>\n      <td>0.071429</td>\n      <td>1.285714</td>\n      <td>0.500000</td>\n      <td>0.132857</td>\n      <td>86.897345</td>\n      <td>45.92459</td>\n      <td>37.785714</td>\n      <td>27.571429</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>True</td>\n      <td>Rio de Janeiro, Brazil</td>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>3</td>\n      <td>187.96</td>\n      <td>Orthodox</td>\n      <td>205.0</td>\n      <td>4.800000</td>\n      <td>3.600000</td>\n      <td>4.700000</td>\n      <td>3.400000</td>\n      <td>29.100000</td>\n      <td>11.300000</td>\n      <td>10.100000</td>\n      <td>6.200000</td>\n      <td>33.500000</td>\n      <td>12.900000</td>\n      <td>0.800000</td>\n      <td>5.600000</td>\n      <td>4.400000</td>\n      <td>0.700000</td>\n      <td>0.100000</td>\n      <td>43.900000</td>\n      <td>20.900000</td>\n      <td>0.503000</td>\n      <td>0.500000</td>\n      <td>1.700000</td>\n      <td>1.300000</td>\n      <td>0.313000</td>\n      <td>72.500000</td>\n      <td>46.200000</td>\n      <td>2.700000</td>\n      <td>1.600000</td>\n      <td>2.400000</td>\n      <td>1.400000</td>\n      <td>21.400000</td>\n      <td>7.200000</td>\n      <td>2.200000</td>\n      <td>1.600000</td>\n      <td>18.300000</td>\n      <td>5.100000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n      <td>3.500000</td>\n      <td>0.500000</td>\n      <td>0.100000</td>\n      <td>26.000000</td>\n      <td>10.200000</td>\n      <td>0.305000</td>\n      <td>0.800000</td>\n      <td>1.200000</td>\n      <td>0.300000</td>\n      <td>0.100000</td>\n      <td>40.200000</td>\n      <td>24.100000</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>18.0</td>\n      <td>396.800000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>False</td>\n      <td>20.0</td>\n      <td>305.000000</td>\n      <td>4.0</td>\n      <td>Middleweight</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>0.973116</td>\n      <td>0.948975</td>\n      <td>0.902913</td>\n      <td>0.270936</td>\n      <td>0.310559</td>\n      <td>0.977444</td>\n      <td>0.811688</td>\n      <td>0.313242</td>\n      <td>0.331010</td>\n      <td>0.592021</td>\n      <td>0.674603</td>\n      <td>0.513458</td>\n      <td>0.647482</td>\n      <td>0.793651</td>\n      <td>0.346320</td>\n      <td>0.383598</td>\n      <td>0.966387</td>\n      <td>0.974026</td>\n      <td>1.430333</td>\n      <td>1.392688</td>\n      <td>1.001652</td>\n      <td>0.952381</td>\n      <td>0.582011</td>\n      <td>0.621118</td>\n      <td>0.911217</td>\n      <td>1.406711</td>\n      <td>1.452297</td>\n      <td>0.752896</td>\n      <td>0.879121</td>\n      <td>2.163866</td>\n      <td>2.410714</td>\n      <td>0.420918</td>\n      <td>0.435540</td>\n      <td>2.410714</td>\n      <td>2.280220</td>\n      <td>0.843819</td>\n      <td>1.358314</td>\n      <td>1.142857</td>\n      <td>0.904762</td>\n      <td>1.047619</td>\n      <td>0.952381</td>\n      <td>0.974026</td>\n      <td>1.561574</td>\n      <td>1.510316</td>\n      <td>1.015809</td>\n      <td>0.595238</td>\n      <td>1.038961</td>\n      <td>1.153846</td>\n      <td>1.029870</td>\n      <td>1.562048</td>\n      <td>1.619928</td>\n      <td>1.0</td>\n      <td>0.666667</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.500000</td>\n      <td>1.105263</td>\n      <td>0.769231</td>\n      <td>5.000000</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>1.142857</td>\n      <td>2.000000</td>\n      <td>2.0</td>\n      <td>1.250000</td>\n    </tr>\n    <tr>\n      <td>3646</td>\n      <td>177.80</td>\n      <td>155.0</td>\n      <td>8.500000</td>\n      <td>4.500000</td>\n      <td>2.500000</td>\n      <td>2.000000</td>\n      <td>43.500000</td>\n      <td>16.000000</td>\n      <td>1.500000</td>\n      <td>1.000000</td>\n      <td>27.500000</td>\n      <td>5.500000</td>\n      <td>0.000000</td>\n      <td>11.500000</td>\n      <td>9.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>47.500000</td>\n      <td>19.000000</td>\n      <td>0.565000</td>\n      <td>71.295658</td>\n      <td>31.374237</td>\n      <td>0.464392</td>\n      <td>1.500000</td>\n      <td>0.537707</td>\n      <td>4.500000</td>\n      <td>1.000000</td>\n      <td>0.110000</td>\n      <td>61.000000</td>\n      <td>31.500000</td>\n      <td>93.753403</td>\n      <td>51.441892</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>3.500000</td>\n      <td>2.000000</td>\n      <td>34.500000</td>\n      <td>3.500000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>36.000000</td>\n      <td>5.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>40.000000</td>\n      <td>6.500000</td>\n      <td>0.085000</td>\n      <td>65.95863</td>\n      <td>27.005665</td>\n      <td>0.414369</td>\n      <td>0.000000</td>\n      <td>0.453785</td>\n      <td>1.000000</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>46.500000</td>\n      <td>13.00000</td>\n      <td>86.383180</td>\n      <td>45.278695</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>False</td>\n      <td>Montreal, Quebec, Canada</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>172.72</td>\n      <td>Southpaw</td>\n      <td>155.0</td>\n      <td>7.750000</td>\n      <td>4.500000</td>\n      <td>9.250000</td>\n      <td>5.000000</td>\n      <td>56.125000</td>\n      <td>14.750000</td>\n      <td>15.375000</td>\n      <td>8.500000</td>\n      <td>66.375000</td>\n      <td>18.000000</td>\n      <td>0.250000</td>\n      <td>6.625000</td>\n      <td>5.750000</td>\n      <td>2.875000</td>\n      <td>0.250000</td>\n      <td>80.750000</td>\n      <td>28.250000</td>\n      <td>0.351250</td>\n      <td>2.000000</td>\n      <td>4.500000</td>\n      <td>2.000000</td>\n      <td>0.355000</td>\n      <td>107.500000</td>\n      <td>52.375000</td>\n      <td>6.125000</td>\n      <td>4.250000</td>\n      <td>5.375000</td>\n      <td>3.125000</td>\n      <td>46.750000</td>\n      <td>11.625000</td>\n      <td>3.250000</td>\n      <td>1.750000</td>\n      <td>48.125000</td>\n      <td>11.250000</td>\n      <td>0.000000</td>\n      <td>1.125000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.375000</td>\n      <td>55.375000</td>\n      <td>16.500000</td>\n      <td>0.288750</td>\n      <td>0.375000</td>\n      <td>2.250000</td>\n      <td>1.250000</td>\n      <td>0.328750</td>\n      <td>82.250000</td>\n      <td>42.125000</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>21.0</td>\n      <td>724.750000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>False</td>\n      <td>4.0</td>\n      <td>393.000000</td>\n      <td>0.0</td>\n      <td>Lightweight</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.029242</td>\n      <td>1.042021</td>\n      <td>1.000000</td>\n      <td>1.085714</td>\n      <td>1.000000</td>\n      <td>0.341463</td>\n      <td>0.500000</td>\n      <td>0.778993</td>\n      <td>1.079365</td>\n      <td>0.152672</td>\n      <td>0.210526</td>\n      <td>0.423006</td>\n      <td>0.342105</td>\n      <td>0.800000</td>\n      <td>1.639344</td>\n      <td>1.481481</td>\n      <td>0.387097</td>\n      <td>0.800000</td>\n      <td>0.593272</td>\n      <td>0.683761</td>\n      <td>1.158187</td>\n      <td>1.121309</td>\n      <td>1.000000</td>\n      <td>0.666667</td>\n      <td>0.819188</td>\n      <td>0.571429</td>\n      <td>0.608899</td>\n      <td>0.280702</td>\n      <td>0.285714</td>\n      <td>0.705882</td>\n      <td>0.727273</td>\n      <td>0.743455</td>\n      <td>0.356436</td>\n      <td>0.705882</td>\n      <td>0.727273</td>\n      <td>0.753181</td>\n      <td>0.489796</td>\n      <td>1.000000</td>\n      <td>1.882353</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.727273</td>\n      <td>0.727273</td>\n      <td>0.428571</td>\n      <td>0.841901</td>\n      <td>1.108284</td>\n      <td>0.615385</td>\n      <td>0.666667</td>\n      <td>1.128881</td>\n      <td>0.570571</td>\n      <td>0.324638</td>\n      <td>1.0</td>\n      <td>0.500000</td>\n      <td>1.0</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>0.227273</td>\n      <td>0.542887</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>0.200000</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n      <td>0.375000</td>\n    </tr>\n  </tbody>\n</table>\n<p>7716 rows  214 columns</p>\n</div>"
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37kat7LT-2-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pdpbox.pdp import pdp_isolate, pdp_plot\n",
        "\n",
        "name = \"Weight_lbs\"\n",
        "\n",
        "isolated = pdp_isolate(\n",
        "        model=model,\n",
        "        dataset=X_test_transformed,\n",
        "        model_features=X_test.columns,\n",
        "        feature=name,\n",
        "        num_grid_points=50,\n",
        ")\n",
        "\n",
        "fig, ax = pdp_plot(isolated, feature_name=name)\n",
        "fig.show()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-54-7fdd34776b3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m isolated = pdp_isolate(\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_test_transformed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mmodel_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzXt6XQywtXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get feature importances\n",
        "rf = pipeline.named_steps['randomforestclassifier']\n",
        "importances = pd.Series(rf.feature_importances_, X_train.columns)\n",
        "\n",
        "# Plot feature importances\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 20\n",
        "plt.figure(figsize=(10,n/2))\n",
        "plt.title(f'Top {n} features')\n",
        "importances.sort_values()[-n:].plot.barh(color='grey');"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFittedError",
          "evalue": "This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-57-02c11d173d8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get feature importances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'randomforestclassifier'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mimportances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Plot feature importances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfeature_importances_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    371\u001b[0m             \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m         \"\"\"\n\u001b[1;32m--> 373\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'estimators_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         all_importances = Parallel(n_jobs=self.n_jobs,\n",
            "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNotFittedError\u001b[0m: This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63fRomEn-oSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # The status_group column is the target\n",
        "# target = 'status_group'\n",
        "\n",
        "# # Get a dataframe with all train columns except the target & id\n",
        "# train_features = train.drop(columns=[target, 'id'])\n",
        "\n",
        "# # Get a list of the numeric features\n",
        "# numeric_features = train_features.select_dtypes(include='number').columns.tolist()\n",
        "\n",
        "# # Get a series with the cardinality of the nonnumeric features\n",
        "# cardinality = train_features.select_dtypes(exclude='number').nunique()\n",
        "\n",
        "# # Get a list of all categorical features with cardinality <= 50\n",
        "# categorical_features = cardinality[cardinality <= 50].index.tolist()\n",
        "\n",
        "# # Combine the lists \n",
        "# features = numeric_features + categorical_features\n",
        "# print(features)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOlQdwR9wvLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Ignore warnings\n",
        "\n",
        "# transformers = make_pipeline(\n",
        "#     ce.OrdinalEncoder(), \n",
        "#     SimpleImputer(strategy='median') \n",
        "# )\n",
        "\n",
        "# X_train_transformed = transformers.fit_transform(X_train)\n",
        "# X_val_transformed = transformers.transform(X_val)\n",
        "\n",
        "# model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "# model.fit(X_train_transformed, y_train)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHC8CS49wwtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "\n",
        "permuter = PermutationImportance(\n",
        "    model,\n",
        "    scoring='accuracy',\n",
        "    n_iter=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "permuter.fit(X_val_transformed, y_val)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-61-05b14fc427a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m permuter = PermutationImportance(\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrWMo5aWwyJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feature_names = X_val.columns.tolist()\n",
        "# pd.Series(permuter.feature_importances_, feature_names).sort_values()"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6fHgnG3wzEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# eli5.show_weights(\n",
        "#     permuter,\n",
        "#     top=None,\n",
        "#     feature_names=feature_names"
      ],
      "execution_count": 63,
      "outputs": []
    }
  ]
}